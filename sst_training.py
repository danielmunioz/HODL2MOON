# -*- coding: utf-8 -*-
"""sst_training.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1B5pVs2K0AmuJptehTAxzPyMqt2F8ZLLe
"""

!pip install datasets
!pip install transformers

import os
import pandas as pd
from collections import OrderedDict

import torch
from torch import nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader

from datasets import load_dataset
from transformers import AdamW, get_scheduler, DistilBertModel, DistilBertConfig, DistilBertTokenizerFast

device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')

#loads only backbone weights weights
os.environ['KAGGLE_CONFIG_DIR'] = '/content'
!kaggle datasets download -d muniozdaniel0/final-distilbert-weight
!unzip ./final-distilbert-weight.zip -d ./final-distilbert-weight
!rm ./final-distilbert-weight.zip

def tokenizer_map(element):
  return tokenizer(element['sentence'], max_length=tokenizer.model_max_length, truncation=True)

class Data_collator():
  def __init__(self, tokenizer): 
    self.tokenizer = tokenizer

  def __call__(self, batch):
    #Assumes we are wotking with a list of dictionaries
    training_dict = {'input_ids': [element['input_ids'] for element in batch], 
                     'attention_mask': [element['attention_mask'] for element in batch]}
    training_samples = self.tokenizer.pad(training_dict, return_tensors = 'pt')
    labels = torch.tensor([element['label'] for element in batch], dtype=torch.float32)

    return {'input_ids': training_samples['input_ids'],
            'attention_mask': training_samples['attention_mask'],
            'labels': labels}

class DistilBertRegressor(nn.Module):
  def __init__(self, weights=None, freeze_backbone=False):
    super().__init__()  
    self.backbone = DistilBertModel(config=DistilBertConfig())
    
    if weights:
      self.backbone.load_state_dict(weights)
      print('Weights loaded succesfully')
    if freeze_backbone:
      for param in self.backbone.parameters():
        param.requires_grad = False

    self.linear =  nn.Linear(self.backbone.config.dim, self.backbone.config.dim)  #'backbone.config.dim' stores the output dimension of the backbone
    self.dropout = nn.Dropout(0.2)
    self.regressor = nn.Linear(self.backbone.config.dim, 1)
  
  #Could be useful to meassure differences? one model using the first layer of a pretrained classifier vs one being randomly initialized.
  def forward(self, input_ids, attention_mask, return_hidden_embeddings=False):
    hidden_embeddings = self.backbone(input_ids=input_ids, attention_mask=attention_mask)[0]
    out = self.linear(hidden_embeddings[:, 0, :])
    out = F.relu(out)
    out = self.dropout(out)
    out = self.regressor(out)

    if not return_hidden_embeddings:
      return out
    else:
      return hidden_embeddings, out

#loading checkpoint
checkpoint = torch.load('./final-distilbert-weight/distilbert_reddit_epoch_2_iter_final.pt', map_location=device)
checkpoint['model'] = OrderedDict({k[11:]: v for k, v in checkpoint['model'].items() if 'distilbert' in k})  #OrderedDict to keep it consistant with pytorch
tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')                    #using ver. 4.10.0.dev0

#load dataset
sst_dataset = load_dataset("sst", "default")
tokenized_sst_dataset = sst_dataset.map(tokenizer_map, remove_columns=['sentence', 'tokens', 'tree'], batched=True)

collate_fn = Data_collator(tokenizer)
sst_train_loader = DataLoader(tokenized_sst_dataset['train'], shuffle=True, batch_size=16, collate_fn=collate_fn)

#Initializing model
#model = DistilBertRegressor(weights=checkpoint['model']).to(device)
model = DistilBertRegressor(weights=checkpoint['model'], freeze_backbone=True).to(device)

#defining loss and optimizer
optim = AdamW(model.parameters(), lr=1e-5)
loss_fn = nn.MSELoss()

num_epochs = 3

num_training_steps = num_epochs * len(sst_train_loader)
lr_scheduler = get_scheduler('linear', optimizer=optim, num_warmup_steps=600, num_training_steps=num_training_steps)

model.train()
for epoch in range(num_epochs):
  for i, batch in enumerate(sst_train_loader):
    input_ids, attention_mask, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['labels'].to(device)
    optim.zero_grad()

    out = model(input_ids, attention_mask, return_hidden_embeddings=False)
    
    #we may need to average the loss over the whole epoch
    loss = loss_fn(out.squeeze(), labels)
    loss.backward()
    optim.step()
    lr_scheduler.step()
    if i!=0 and i%200 == 0:
      print('saving model at iter{}'.format(i))
      
      torch.save({'epoch': epoch,
                  'model_state_dict':model.state_dict(),
                  'optimizer_state_dict':optim.state_dict(),
                  'lr_scheduler':lr_scheduler.state_dict(),
                  'local_loss':loss.item()},
                  './Distilbert_reddit_sst_weights_frozenbackbone/Distilbert_reddit_sst_epoch_{}_iter_{}.pt'.format(epoch, i))
    if i!=0 and i%100==0:
      print('epoch: {} iter: {} loss: {}'.format(epoch, i, loss.item()))

  print('storing "end of the epoch" weights')
  torch.save({'epoch': epoch,
              'model_state_dict':model.state_dict(),
              'optimizer_state_dict':optim.state_dict(),
              'lr_scheduler':lr_scheduler.state_dict(),
              'local_loss':loss.item()},
              './Distilbert_reddit_sst_weights_frozenbackbone/Distilbert_reddit_sst_epoch_{}.pt'.format(epoch))

"""####Testing"""

#loading weights
os.environ['KAGGLE_CONFIG_DIR'] = '/content'
!kaggle datasets download -d muniozdaniel0/distilbert-reddit-sst-weights-frozenbackbone-2
!unzip ./distilbert-reddit-sst-weights-frozenbackbone-2.zip -d ./distilbert-reddit-sst-weights-frozenbackbone-2
!rm ./distilbert-reddit-sst-weights-frozenbackbone-2.zip

#regressor values test
#loading sst_frozen checkpoint
sst_frozen_checkpoint = torch.load('./distilbert-reddit-sst-weights-frozenbackbone-2/Distilbert_reddit_sst_epoch_2.pt', map_location=device)
tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')                    #using ver. 4.10.0.dev0

#loading model
model = DistilBertRegressor().to(device)
model.load_state_dict(sst_frozen_checkpoint['model_state_dict'])
model.eval()

#load dataset
sst_dataset = load_dataset("sst", "default")
tokenized_sst_dataset = sst_dataset.map(tokenizer_map, remove_columns=['sentence', 'tokens', 'tree'], batched=True)

collate_fn = Data_collator(tokenizer)
sst_test_loader = DataLoader(tokenized_sst_dataset['test'], shuffle=False, batch_size=16, collate_fn=collate_fn)

with torch.no_grad():
  cumulative_loss = 0.0
  out_values = torch.tensor([])
  labels_values = torch.Tensor([])
  for _, batch in enumerate(sst_test_loader):
    out = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])
    local_loss = nn.MSELoss()(out.view(-1), batch['labels'])
  
    cumulative_loss+=local_loss
    out_values = torch.cat((out_values, out.view(-1)))
    labels_values = torch.cat((labels_values, batch['labels']))

    print(local_loss, out.shape)
  global_eval_loss = cumulative_loss/len(sst_test_loader)

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.pyplot import figure

dummy_y = np.zeros_like(out_values.numpy())
out_values.min(), out_values.max()

figure(figsize=(24, 6), dpi=80)
plt.scatter(labels_values.numpy(), dummy_y)

