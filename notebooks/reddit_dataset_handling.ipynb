{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "reddit_dataset_handling.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Px2Qd6xzFKgL"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install randomdict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcGHCudxFMf7"
      },
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "from randomdict import RandomDict\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from datasets import load_dataset\n",
        "from transformers import DistilBertTokenizerFast, DistilBertForMaskedLM, DataCollatorWithPadding, DataCollatorForLanguageModeling"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HmOg5dDnCXn"
      },
      "source": [
        "#kaggle creation\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = '/content'\n",
        "!kaggle datasets init -p '/content/reddit_test_dataset'\n",
        "!kaggle datasets create -p '/content/reddit_train_dataset'\n",
        "\n",
        "!kaggle datasets init -p '/content/reddit'\n",
        "!kaggle datasets create -p '/content/reddit_train_dataset'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDHdhTUdFF0a"
      },
      "source": [
        "def tokenize_fn(element):\n",
        "  sample = tokenizer(element['content'], truncation=True, return_special_tokens_mask=True)\n",
        "\n",
        "  ids = np.array(sample['input_ids'])\n",
        "  tokens_mask = np.array(sample['special_tokens_mask'])\n",
        "  labels_test = sample['input_ids']\n",
        "  attention_mask = sample['attention_mask']\n",
        "\n",
        "  rand_array = np.random.rand(*ids.shape)\n",
        "  mask_array = (rand_array<.15) * ~tokens_mask.astype(bool)\n",
        "\n",
        "  mask_idx = np.where(mask_array)[0]\n",
        "  rand_mask = np.random.rand(*mask_idx.shape)\n",
        "  mask_positions = mask_idx[rand_mask<.8]\n",
        "  mask_random_word = mask_idx[(.8<=rand_mask) & (rand_mask<.9)]\n",
        "\n",
        "  ids[mask_positions] = tokenizer.mask_token_id\n",
        "  for elemen in mask_random_word:\n",
        "    ids[elemen] = random_vocab.random_value()\n",
        "\n",
        "  ids = ids.tolist()\n",
        "\n",
        "  return {'input_ids': ids, 'attention_mask': attention_mask, 'labels': labels_test}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGvEJVliFQEq"
      },
      "source": [
        "#dataset_creation\n",
        "random_vocab = RandomDict(tokenizer.vocab)\n",
        "del random_vocab['[CLS]'], random_vocab['[MASK]'], random_vocab['[PAD]'], random_vocab['[SEP]'], random_vocab['[UNK]']\n",
        "\n",
        "dataset_train = load_dataset('reddit', split='train[:70%]')\n",
        "tokenized_train = dataset_train.map(tokenize_fn, remove_columns=['author', 'body', 'id', 'normalizedBody', 'subreddit', 'summary', 'subreddit_id', 'content'], batched=False)\n",
        "tokenized_train.save_to_disk('reddit_train_dataset')\n",
        "\n",
        "dataset_test = load_dataset('reddit', split='train[30%:]')\n",
        "tokenized_test = dataset_test.map(tokenize_fn, remove_columns=['author', 'body', 'id', 'normalizedBody', 'subreddit', 'summary', 'subreddit_id', 'content'], batched=False)\n",
        "tokenized_test.save_to_disk('reddit_test_dataset')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxhrsffqnFTT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}