{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sst_training.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "P5ECi2YqXsip",
        "ELnuFNtaX7YK",
        "YkLymsiHeBh6",
        "NpYMwvQMpPZ4",
        "qRzrT2f_IwWk",
        "K995P33epYaF",
        "wJb6n00Uur2x",
        "X5dEHe4J09Pv",
        "aJ3zYgxF9Dyy"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rmr-dcyX1GrG"
      },
      "source": [
        "!pip install datasets\n",
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVktkUt4en7q"
      },
      "source": [
        "import os\n",
        "import requests\n",
        "from collections import OrderedDict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from datasets import load_dataset\n",
        "from transformers import AdamW, get_scheduler, DistilBertModel, DistilBertConfig, DistilBertTokenizerFast\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAYUJwp62HBU"
      },
      "source": [
        "#loads only backbone weights\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = './'\n",
        "!kaggle datasets download -d muniozdaniel0/final-distilbert-weight\n",
        "!unzip ./final-distilbert-weight.zip -d ./final-distilbert-weight\n",
        "!rm ./final-distilbert-weight.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVhHrpogGR4k"
      },
      "source": [
        "#loading backbone checkpoint ----may be a good idea to change it to 'backbone_checkpoint'\n",
        "checkpoint = torch.load('./final-distilbert-weight/distilbert_reddit_epoch_2_iter_final.pt', map_location=device)\n",
        "checkpoint['model'] = OrderedDict({k[11:]: v for k, v in checkpoint['model'].items() if 'distilbert' in k})  #OrderedDict to keep it consistant with pytorch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5ECi2YqXsip"
      },
      "source": [
        "###Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdonTtjXp3ul"
      },
      "source": [
        "##sst-2 regresor\n",
        "def tokenizer_map(element):\n",
        "  return tokenizer(element['sentence'], max_length=tokenizer.model_max_length, truncation=True)\n",
        "\n",
        "class Data_collator():\n",
        "  def __init__(self, tokenizer): \n",
        "    self.tokenizer = tokenizer\n",
        "\n",
        "  def __call__(self, batch):\n",
        "    #Assumes we are wotking with a list of dictionaries\n",
        "    training_dict = {'input_ids': [element['input_ids'] for element in batch], \n",
        "                     'attention_mask': [element['attention_mask'] for element in batch]}\n",
        "    training_samples = self.tokenizer.pad(training_dict, return_tensors = 'pt')\n",
        "    labels = torch.tensor([element['label'] for element in batch], dtype=torch.float32)\n",
        "\n",
        "    return {'input_ids': training_samples['input_ids'],\n",
        "            'attention_mask': training_samples['attention_mask'],\n",
        "            'labels': labels}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WL3rHesspyGp"
      },
      "source": [
        "##sst-2 binary\n",
        "def tokenizer_map_df(element):\n",
        "  #sst-2 tokenizer_df\n",
        "  tokenized_elemen = tokenizer(element['sentence'], max_length=tokenizer.model_max_length, truncation=True)\n",
        "  return element.append(pd.Series([tokenized_elemen['input_ids'], tokenized_elemen['attention_mask']], index=['input_ids', 'attention_mask']))\n",
        "  \n",
        "class Data_collator():\n",
        "  def __init__(self, tokenizer): \n",
        "    self.tokenizer = tokenizer\n",
        "\n",
        "  def __call__(self, batch):\n",
        "    #Assumes we are wotking with a list of dictionaries\n",
        "    training_dict = {'input_ids': [element['input_ids'] for element in batch], \n",
        "                     'attention_mask': [element['attention_mask'] for element in batch]}\n",
        "    training_samples = self.tokenizer.pad(training_dict, return_tensors = 'pt')\n",
        "    labels = torch.tensor([element['label'] for element in batch], dtype=torch.float32)\n",
        "\n",
        "    return {'input_ids': training_samples['input_ids'],\n",
        "            'attention_mask': training_samples['attention_mask'],\n",
        "            'labels': labels}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPWzUheHb9-6"
      },
      "source": [
        "##financial phrasebank\n",
        "def tokenizer_map_df(element):\n",
        "  #sst-2 tokenizer_df\n",
        "  tokenized_elemen = tokenizer(element['sentence'], max_length=tokenizer.model_max_length, truncation=True)\n",
        "  return element.append(pd.Series([tokenized_elemen['input_ids'], tokenized_elemen['attention_mask']], index=['input_ids', 'attention_mask']))\n",
        "\n",
        "class Data_collator():\n",
        "  def __init__(self, tokenizer): \n",
        "    self.tokenizer = tokenizer\n",
        "\n",
        "  def __call__(self, batch):\n",
        "    #Assumes we are wotking with a list of dictionaries\n",
        "    training_dict = {'input_ids': [element['input_ids'] for element in batch], \n",
        "                     'attention_mask': [element['attention_mask'] for element in batch]}\n",
        "    training_samples = self.tokenizer.pad(training_dict, return_tensors = 'pt')\n",
        "    labels = torch.tensor([element['label'] for element in batch], dtype=torch.float32)\n",
        "\n",
        "    return {'input_ids': training_samples['input_ids'],\n",
        "            'attention_mask': training_samples['attention_mask'],\n",
        "            'labels': labels}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vF85u5r4X0Fy"
      },
      "source": [
        "##sentiment 140\n",
        "#### -----current version\n",
        "class tokenizer_map_df():\n",
        "  def __init__(self, tokenizer, column_name=None):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.column_name = column_name\n",
        "  \n",
        "  def __call__(self, element):\n",
        "    if not self.column_name:\n",
        "      tokenized_elemen = tokenizer(element['sentence'], max_length=tokenizer.model_max_length, truncation=True)\n",
        "    else:\n",
        "      tokenized_elemen = tokenizer(element[self.column_name], max_length=tokenizer.model_max_length, truncation=True)\n",
        "\n",
        "    return element.append(pd.Series([tokenized_elemen['input_ids'], tokenized_elemen['attention_mask']], index=['input_ids', 'attention_mask']))\n",
        "\n",
        "### -----reads array_strings as arrays\n",
        "def str2arrray(elemen):\n",
        "  return np.fromstring(elemen[1:-1], sep=',', dtype=np.int64)\n",
        "  \n",
        "\n",
        "### -----current version\n",
        "class Data_collator():\n",
        "  #List of dictionaries version\n",
        "  def __init__(self, tokenizer): \n",
        "    self.tokenizer = tokenizer\n",
        "\n",
        "  def __call__(self, batch):\n",
        "    training_dict = {'input_ids': [element['input_ids'] for element in batch], \n",
        "                     'attention_mask': [element['attention_mask'] for element in batch]}\n",
        "    training_samples = self.tokenizer.pad(training_dict, return_tensors = 'pt')\n",
        "    labels = torch.tensor([element['label'] for element in batch], dtype=torch.float32)\n",
        "\n",
        "    return {'input_ids': training_samples['input_ids'],\n",
        "            'attention_mask': training_samples['attention_mask'],\n",
        "            'labels': labels}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELnuFNtaX7YK"
      },
      "source": [
        "###Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O17KGDdrpvmd"
      },
      "source": [
        "class sst_binary_dataset(Dataset):\n",
        "  def __init__(self, dataframe):\n",
        "    self.df = dataframe\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.df)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    return {'input_ids': self.df.iloc[idx]['input_ids'], \n",
        "            'attention_mask': self.df.iloc[idx]['attention_mask'],\n",
        "            'label': self.df.iloc[idx]['label']}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqAWF2-8b4OD"
      },
      "source": [
        "class fp_dataset(Dataset):\n",
        "  def __init__(self, dataframe):\n",
        "    self.df = dataframe\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.df)\n",
        "    \n",
        "  def __getitem__(self, idx): \n",
        "    return {'input_ids': self.df['input_ids'].iloc[idx],\n",
        "            'attention_mask': self.df['attention_mask'].iloc[idx],\n",
        "            'label': self.df['label'].iloc[idx]}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4eBHDrwCX-lH"
      },
      "source": [
        "class Sentiment140(Dataset):\n",
        "  def __init__(self, dataframe):\n",
        "    self.df = dataframe\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.df)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return {'input_ids': self.df['input_ids'].iloc[idx],\n",
        "            'attention_mask': self.df['attention_mask'].iloc[idx],\n",
        "            'label': self.df['label'].iloc[idx]}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPnbFwGCqX-5"
      },
      "source": [
        "class df_dataset(Dataset):\n",
        "  def __init__(self, dataframe, label_name=None):\n",
        "    \"\"\"\n",
        "    Initializes a torch.dataset from a dataframe that contains the columns: 'input_ids', 'attention_mask', 'label'\n",
        "\n",
        "    Args:\n",
        "      dataframe: A pandas dataframe\n",
        "    \"\"\"\n",
        "    self.df = dataframe\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.df)\n",
        "  \n",
        "  def __getitem(self, idx):\n",
        "    return {'input_ids': self.df['input_ids'].iloc[idx],\n",
        "            'attention_mask': self.df['attention_mask'].iloc[idx],\n",
        "            'label': self.df[label_name].iloc[idx] if label_name else self.df['label'].iloc[idx]}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkLymsiHeBh6"
      },
      "source": [
        "###Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edLy8KiaeDOO"
      },
      "source": [
        "class DistilBertRegressor(nn.Module):\n",
        "  def __init__(self, backbone_weights=None, freeze_backbone=False):\n",
        "    super().__init__()  \n",
        "    self.backbone = DistilBertModel(config=DistilBertConfig())\n",
        "    \n",
        "    if backbone_weights:\n",
        "      self.backbone.load_state_dict(backbone_weights)\n",
        "      print('backbone weights loaded succesfully')\n",
        "    if freeze_backbone:\n",
        "      for param in self.backbone.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    self.linear =  nn.Linear(self.backbone.config.dim, self.backbone.config.dim)  #'backbone.config.dim' stores the output dimension of the backbone\n",
        "    self.dropout = nn.Dropout(0.2)\n",
        "    self.regressor = nn.Linear(self.backbone.config.dim, 1)\n",
        "  \n",
        "  #Could be useful to meassure differences? one model using the first layer of a pretrained classifier vs one being randomly initialized.\n",
        "  def forward(self, input_ids, attention_mask, return_hidden_embeddings=False):\n",
        "    hidden_embeddings = self.backbone(input_ids=input_ids, attention_mask=attention_mask)[0]\n",
        "    out = self.linear(hidden_embeddings[:, 0, :])\n",
        "    out = F.relu(out)\n",
        "    out = self.dropout(out)\n",
        "    out = self.regressor(out)\n",
        "\n",
        "    if not return_hidden_embeddings:\n",
        "      return out\n",
        "    else:\n",
        "      return hidden_embeddings, out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24sfkiOveDYo"
      },
      "source": [
        "#### ---Current Classifier Version---\n",
        "class DistilBertClassifier(nn.Module):\n",
        "  def __init__(self, out_dim=2, backbone_weights=None, freeze_backbone=False):\n",
        "    super().__init__()  \n",
        "    self.backbone = DistilBertModel(config=DistilBertConfig())\n",
        "    \n",
        "    if backbone_weights:\n",
        "      self.backbone.load_state_dict(backbone_weights)\n",
        "      print('backbone weights loaded succesfully')\n",
        "    if freeze_backbone:\n",
        "      for param in self.backbone.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    self.linear =  nn.Linear(self.backbone.config.dim, self.backbone.config.dim)  #'backbone.config.dim' stores the output dimension of the backbone\n",
        "    self.dropout = nn.Dropout(0.2)\n",
        "    self.classifier = nn.Linear(self.backbone.config.dim, out_dim)\n",
        "  \n",
        "  def forward(self, input_ids, attention_mask, return_hidden_embeddings=False):\n",
        "    hidden_embeddings = self.backbone(input_ids=input_ids, attention_mask=attention_mask)[0]\n",
        "    out = self.linear(hidden_embeddings[:, 0, :])\n",
        "    out = F.relu(out)\n",
        "    out = self.dropout(out)\n",
        "    out = self.classifier(out)\n",
        "\n",
        "    if not return_hidden_embeddings:\n",
        "      return out\n",
        "    else:\n",
        "      return hidden_embeddings, out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpYMwvQMpPZ4"
      },
      "source": [
        "###sst-2 Regressor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJm6DBx1l1b-"
      },
      "source": [
        "#defining tokenizer and DataCollator\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')                    #using ver. 4.10.0.dev0\n",
        "collate_fn = Data_collator(tokenizer)\n",
        "\n",
        "#load sst-2 dataset\n",
        "sst_dataset = load_dataset(\"sst\", \"default\")\n",
        "\n",
        "tokenized_sst_dataset = sst_dataset.map(tokenizer_map, remove_columns=['sentence', 'tokens', 'tree'], batched=True)\n",
        "sst_train_loader = DataLoader(tokenized_sst_dataset['train'], shuffle=True, batch_size=16, collate_fn=collate_fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cncvOH5s1zxg"
      },
      "source": [
        "#Initializing model\n",
        "#model = DistilBertRegressor(backbone_weights=checkpoint['model']).to(device)\n",
        "model = DistilBertRegressor(backbone_weights=checkpoint['model'], freeze_backbone=True).to(device)\n",
        "\n",
        "#defining loss and optimizer\n",
        "optim = AdamW(model.parameters(), lr=1e-5)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "num_epochs = 3\n",
        "\n",
        "num_training_steps = num_epochs * len(sst_train_loader)\n",
        "lr_scheduler = get_scheduler('linear', optimizer=optim, num_warmup_steps=600, num_training_steps=num_training_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1E7b__Q2IGiC"
      },
      "source": [
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "  for i, batch in enumerate(sst_train_loader):\n",
        "    input_ids, attention_mask, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['labels'].to(device)\n",
        "    optim.zero_grad()\n",
        "\n",
        "    out = model(input_ids, attention_mask, return_hidden_embeddings=False)\n",
        "    \n",
        "    #we may need to average the loss over the whole epoch\n",
        "    loss = loss_fn(out.squeeze(), labels)\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "    lr_scheduler.step()\n",
        "    if i!=0 and i%200 == 0:\n",
        "      print('saving model at iter{}'.format(i))\n",
        "      \n",
        "      torch.save({'epoch': epoch,\n",
        "                  'model_state_dict':model.state_dict(),\n",
        "                  'optimizer_state_dict':optim.state_dict(),\n",
        "                  'lr_scheduler':lr_scheduler.state_dict(),\n",
        "                  'local_loss':loss.item()},\n",
        "                  './Distilbert_reddit_sst_weights_frozenbackbone/Distilbert_reddit_sst_epoch_{}_iter_{}.pt'.format(epoch, i))\n",
        "    if i!=0 and i%100==0:\n",
        "      print('epoch: {} iter: {} loss: {}'.format(epoch, i, loss.item()))\n",
        "\n",
        "  print('storing \"end of the epoch\" weights')\n",
        "  torch.save({'epoch': epoch,\n",
        "              'model_state_dict':model.state_dict(),\n",
        "              'optimizer_state_dict':optim.state_dict(),\n",
        "              'lr_scheduler':lr_scheduler.state_dict(),\n",
        "              'local_loss':loss.item()},\n",
        "              './Distilbert_reddit_sst_weights_frozenbackbone/Distilbert_reddit_sst_epoch_{}.pt'.format(epoch))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRzrT2f_IwWk"
      },
      "source": [
        "####Regressor Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsxMy9cP8o7w"
      },
      "source": [
        "#loading weights\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = '/content'\n",
        "!kaggle datasets download -d muniozdaniel0/distilbert-reddit-sst-weights-frozenbackbone-2\n",
        "!unzip ./distilbert-reddit-sst-weights-frozenbackbone-2.zip -d ./distilbert-reddit-sst-weights-frozenbackbone-2\n",
        "!rm ./distilbert-reddit-sst-weights-frozenbackbone-2.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCJEPBflhPm4"
      },
      "source": [
        "#regressor values test\n",
        "#loading sst_frozen checkpoint\n",
        "sst_frozen_checkpoint = torch.load('./distilbert-reddit-sst-weights-frozenbackbone-2/Distilbert_reddit_sst_epoch_2.pt', map_location=device)\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')                    #using ver. 4.10.0.dev0\n",
        "\n",
        "#loading model\n",
        "model = DistilBertRegressor().to(device)\n",
        "model.load_state_dict(sst_frozen_checkpoint['model_state_dict'])\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRBdhD69j1ox"
      },
      "source": [
        "#load dataset\n",
        "sst_dataset = load_dataset(\"sst\", \"default\")\n",
        "tokenized_sst_dataset = sst_dataset.map(tokenizer_map, remove_columns=['sentence', 'tokens', 'tree'], batched=True)\n",
        "\n",
        "collate_fn = Data_collator(tokenizer)\n",
        "sst_test_loader = DataLoader(tokenized_sst_dataset['test'], shuffle=False, batch_size=16, collate_fn=collate_fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlKyItDGrkOt"
      },
      "source": [
        "with torch.no_grad():\n",
        "  cumulative_loss = 0.0\n",
        "  out_values = torch.tensor([])\n",
        "  labels_values = torch.Tensor([])\n",
        "  for _, batch in enumerate(sst_test_loader):\n",
        "    out = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n",
        "    local_loss = nn.MSELoss()(out.view(-1), batch['labels'])\n",
        "  \n",
        "    cumulative_loss+=local_loss\n",
        "    out_values = torch.cat((out_values, out.view(-1)))\n",
        "    labels_values = torch.cat((labels_values, batch['labels']))\n",
        "\n",
        "    print(local_loss, out.shape)\n",
        "  global_eval_loss = cumulative_loss/len(sst_test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwKN3-YI_Sji"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "dummy_y = np.zeros_like(out_values.numpy())\n",
        "out_values.min(), out_values.max()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431
        },
        "id": "je6Y3EKaJPup",
        "outputId": "a9550eae-fb44-4f86-fb13-b2cc062ceebc"
      },
      "source": [
        "figure(figsize=(24, 6), dpi=80)\n",
        "plt.scatter(labels_values.numpy(), dummy_y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7f8554d5a7d0>"
            ]
          },
          "metadata": {},
          "execution_count": 156
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1920x480 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K995P33epYaF"
      },
      "source": [
        "###sst-2 binary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSZpp019pag4",
        "outputId": "94520a2f-b544-4668-d4a5-c754c5713f68"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGBEmdIMHUTx"
      },
      "source": [
        "#downloading binary sst-2\n",
        "data_dir = tf.keras.utils.get_file(\n",
        "    fname='SST-2.zip', \n",
        "    origin='https://dl.fbaipublicfiles.com/glue/data/SST-2.zip',\n",
        "    extract=True)\n",
        "data_dir = os.path.splitext(data_dir)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRd47fMnal6l"
      },
      "source": [
        "#defining tokenizer and dcollator\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased') #using ver. 4.10.0.dev0\n",
        "collate_fn = Data_collator(tokenizer)\n",
        "\n",
        "#loading dataframes\n",
        "train_df = pd.read_csv(os.path.join(data_dir, 'train.tsv'), sep='\\t')\n",
        "dev_df = pd.read_csv(os.path.join(data_dir, 'dev.tsv'), sep='\\t')\n",
        "test_df = pd.read_csv(os.path.join(data_dir, 'test.tsv'), sep='\\t')\n",
        "\n",
        "#tokenizing data ---only train data\n",
        "tokenized_train_df = train_df.apply(tokenizer_map_df, axis=1)\n",
        "tokenized_train_df = tokenized_train_df[['sentence', 'input_ids', 'attention_mask', 'label']]  #changing columns order\n",
        "\n",
        "#dataset and dataloader ---only train data\n",
        "train_dset = sst_binary_dataset(tokenized_train_df)\n",
        "train_loader = DataLoader(train_dset, shuffle=True, batch_size=16, collate_fn=collate_fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUYe-Gv1meoF",
        "outputId": "35f3ab5d-3329-42b2-809f-0b0b8d937859"
      },
      "source": [
        "#Loading model and tokenizer\n",
        "#model = DistilBertClassifier(backbone_weights=checkpoint['model']).to(device) #no frozen layers first\n",
        "model = DistilBertClassifier(backbone_weights=checkpoint['model'], freeze_backbone=True).to(device) #freezeing layers\n",
        "\n",
        "#defining loss and optimizer\n",
        "optim = AdamW(model.parameters(), lr=1e-5)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "num_epochs = 3\n",
        "num_training_steps = num_epochs * len(train_loader)\n",
        "lr_scheduler = get_scheduler('linear', optimizer=optim, num_warmup_steps=600, num_training_steps=num_training_steps)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights loaded succesfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QBmHCjUH25o"
      },
      "source": [
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "  epoch_loss = 0.0\n",
        "  running_loss = 0.0\n",
        "  for i, batch in enumerate(train_loader):\n",
        "    input_ids, attention_mask, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['labels']\n",
        "    optim.zero_grad()\n",
        "\n",
        "    out = model(input_ids, attention_mask, return_hidden_embeddings=False)\n",
        "    loss = loss_fn(out, labels.type(torch.LongTensor).to(device))\n",
        "\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "    lr_scheduler.step()\n",
        "\n",
        "    epoch_loss+=loss.item()\n",
        "    running_loss+=loss.item()\n",
        "    if i!=0 and i%2000 == 0:\n",
        "      print('saving model at iter{}'.format(i))\n",
        "      torch.save({'epoch': epoch,\n",
        "                  'model_state_dict':model.state_dict(),\n",
        "                  'optimizer_state_dict':optim.state_dict(),\n",
        "                  'lr_scheduler':lr_scheduler.state_dict(),\n",
        "                  'running_loss':running_loss/2000},\n",
        "                  './distilbert-frozenweights-sst-binary-shuffle-true/Distilbert_reddit_sst_binary_epoch_{}_iter_{}.pt'.format(epoch, i))\n",
        "      \n",
        "      print('epoch: {}_final running_loss: {}'.format(epoch, running_loss/2000))\n",
        "      running_loss = 0.0\n",
        "      \n",
        "    if i!=0 and i%100==0:\n",
        "      print('epoch: {} iter: {} last_batch_loss: {}'.format(epoch, i, loss.item()))\n",
        "\n",
        "  epoch_loss = epoch_loss/len(train_loader)\n",
        "  print('storing \"end of the epoch\" weights epoch_loss: {}'.format(epoch_loss))\n",
        "  torch.save({'epoch': epoch,\n",
        "              'model_state_dict':model.state_dict(),\n",
        "              'optimizer_state_dict':optim.state_dict(),\n",
        "              'lr_scheduler':lr_scheduler.state_dict(),\n",
        "              'epoch_loss':epoch_loss},\n",
        "              './distilbert-frozenweights-sst-binary-shuffle-true/Distilbert_reddit_sst_binary_epoch_{}.pt'.format(epoch))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJb6n00Uur2x"
      },
      "source": [
        "###Financial Phrasebank\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJsm5fY_59DL"
      },
      "source": [
        "#downloading data\n",
        "url = 'https://www.researchgate.net/profile/Pekka-Malo/publication/251231364_FinancialPhraseBank-v10/data/0c96051eee4fb1d56e000000/FinancialPhraseBank-v10.zip'\n",
        "r = requests.get(url)\n",
        "with open('./FinancialPhraseBank-v10.zip', 'wb') as file:\n",
        "  file.write(r.content)\n",
        "!unzip ./FinancialPhraseBank-v10.zip -d FinancialPhraseBank-v10\n",
        "!rm ./FinancialPhraseBank-v10.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vu3iTm7EvC8f"
      },
      "source": [
        "#Loading financial phrasebank dataset\n",
        "#fp_dataframe_allagree = pd.read_csv('./FinancialPhraseBank-v10/FinancialPhraseBank-v1.0/Sentences_AllAgree.txt', sep='@', names=['sentence', 'label'], engine='python')\n",
        "fp_dataframe_50agree = pd.read_csv('./FinancialPhraseBank-v10/FinancialPhraseBank-v1.0/Sentences_50Agree.txt', sep='@', names=['sentence', 'label'], engine='python')\n",
        "\n",
        "#mapping data - allagree\n",
        "label_mapping = {'negative':0, 'neutral':1, 'positive':2}\n",
        "fp_dataframe_50agree['label'] = fp_dataframe_50agree['label'].map(label_mapping)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMWGDqWNvHZf"
      },
      "source": [
        "#defining tokenizer and Data Collator\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased') #using ver. 4.10.0.dev0\n",
        "collate_fn = Data_collator(tokenizer)\n",
        "\n",
        "#tokenizing data\n",
        "tokenized_fp_dataframe_50agree = fp_dataframe_50agree.apply(tokenizer_map_df, axis=1)\n",
        "tokenized_fp_dataframe_50agree = tokenized_fp_dataframe_50agree[['sentence', 'input_ids', 'attention_mask', 'label']]\n",
        "\n",
        "#splitting dataframe\n",
        "fp_train, fp_test = train_test_split(tokenized_fp_dataframe_50agree, test_size=0.2, random_state=2332)\n",
        "fp_train, fp_val = train_test_split(fp_train, test_size=0.1, random_state=2332)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nUi0Lwr4pi6"
      },
      "source": [
        "#datasets+dataloaders\n",
        "fp_dataset_train = fp_dataset(fp_train)\n",
        "fp_dataset_test = fp_dataset(fp_test)\n",
        "fp_dataset_val = fp_dataset(fp_val)\n",
        "\n",
        "train_loader = DataLoader(fp_dataset_train, shuffle=True, batch_size=16, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(fp_dataset_test, shuffle=True, batch_size=16, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(fp_dataset_val, shuffle=True, batch_size=16, collate_fn=collate_fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFbTWgTZ8AxU"
      },
      "source": [
        "###---Training\n",
        "\n",
        "#Loading model \n",
        "model = DistilBertClassifier(backbone_weights=checkpoint['model'], out_dim=3).to(device) #no frozen layers first\n",
        "#model = DistilBertClassifier(backbone_weights=checkpoint['model'],  out_dim=3, freeze_backbone=True).to(device) #freezeing layers\n",
        "\n",
        "#defining loss and optimizer\n",
        "optim = AdamW(model.parameters(), lr=1e-5)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "num_epochs = 3\n",
        "num_training_steps = num_epochs * len(train_loader)\n",
        "lr_scheduler = get_scheduler('linear', optimizer=optim, num_warmup_steps=600, num_training_steps=num_training_steps)\n",
        "\n",
        "\n",
        "\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "  epoch_loss = 0.0\n",
        "  running_loss = 0.0\n",
        "  for i, batch in enumerate(train_loader):\n",
        "    input_ids, attention_mask, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['labels']\n",
        "    optim.zero_grad()\n",
        "\n",
        "    out = model(input_ids, attention_mask, return_hidden_embeddings=False)\n",
        "    loss = loss_fn(out, labels.type(torch.LongTensor).to(device))\n",
        "\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "    lr_scheduler.step()\n",
        "\n",
        "    epoch_loss+=loss.item()\n",
        "    running_loss+=loss.item()\n",
        "    if i!=0 and i%2000 == 0:\n",
        "      print('saving model at iter{}'.format(i))\n",
        "      torch.save({'epoch': epoch,\n",
        "                  'model_state_dict':model.state_dict(),\n",
        "                  'optimizer_state_dict':optim.state_dict(),\n",
        "                  'lr_scheduler':lr_scheduler.state_dict(),\n",
        "                  'running_loss':running_loss/2000},\n",
        "                  './distilbert-reddit-financial-phrasebank-3-epochs/Distilbert_reddit_financial_phrasebank_epoch_{}_iter_{}.pt'.format(epoch, i))\n",
        "      \n",
        "      print('epoch: {}_final running_loss: {}'.format(epoch, running_loss/2000))\n",
        "      running_loss = 0.0\n",
        "      \n",
        "    if i!=0 and i%100==0:\n",
        "      print('epoch: {} iter: {} last_batch_loss: {}'.format(epoch, i, loss.item()))\n",
        "\n",
        "  epoch_loss = epoch_loss/len(train_loader)\n",
        "  print('storing \"end of the epoch\" weights epoch_loss: {}'.format(epoch_loss))\n",
        "  torch.save({'epoch': epoch,\n",
        "              'model_state_dict':model.state_dict(),\n",
        "              'optimizer_state_dict':optim.state_dict(),\n",
        "              'lr_scheduler':lr_scheduler.state_dict(),\n",
        "              'epoch_loss':epoch_loss},\n",
        "              './distilbert-reddit-financial-phrasebank-3-epochs/Distilbert_reddit_financial_phrasebank_epoch_{}.pt'.format(epoch))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5dEHe4J09Pv"
      },
      "source": [
        "####Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHbCTzIOyuPJ"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "#loading weights\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = '/content'\n",
        "!kaggle datasets download -d muniozdaniel0/distilbert-reddit-financial-phrasebank-allagree\n",
        "!unzip ./distilbert-reddit-financial-phrasebank-allagree.zip -d ./distilbert-reddit-fp-allagree\n",
        "!rm ./distilbert-reddit-financial-phrasebank-allagree.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKccwswvHgVP"
      },
      "source": [
        "def testing(model, data_loader, return_probs=False, keep_training=False, verbose=False):\n",
        "  loss = 0.0\n",
        "  accuracy = 0.0\n",
        "\n",
        "  labels = torch.tensor([], dtype=torch.double)\n",
        "  predictions = torch.tensor([], dtype=torch.double)\n",
        "  probabilities = torch.tensor([], dtype=torch.double)\n",
        "  \n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for i, elemen in enumerate(data_loader):\n",
        "      out_model = model(elemen['input_ids'], elemen['attention_mask'])\n",
        "      batch_loss = F.cross_entropy(out_model, elemen['labels'].type(torch.LongTensor))\n",
        "\n",
        "      probs = F.softmax(out_model, dim=1)\n",
        "      vals, idx = probs.topk(1, dim=1)\n",
        "      acc = sum(idx.squeeze() == elemen['labels'].type(torch.LongTensor))/len(idx.squeeze())\n",
        "\n",
        "      labels = torch.cat((labels, elemen['labels'].type(torch.LongTensor)))\n",
        "      predictions = torch.cat((predictions, idx.squeeze()))\n",
        "      probabilities = torch.cat((probabilities, vals.squeeze()))\n",
        "\n",
        "      loss+=batch_loss.item()\n",
        "      accuracy+=acc.item()\n",
        "\n",
        "      if verbose:\n",
        "        #print('step: {}, accuracy: {}, loss: {}'.format(i+1, accuracy/(i+1), loss/(i+1)))\n",
        "        print('step: {}, batch_loss: {}, batch_accuracy: {}'.format(i+1, batch_loss.item(), acc.item()))\n",
        "      \n",
        "  loss/=len(data_loader)\n",
        "  accuracy/=len(data_loader)\n",
        "  print('---- Total accuracy: {}, Total loss: {}'.format(accuracy, loss))\n",
        "  if keep_training:\n",
        "    model.train()\n",
        "\n",
        "  if not return_probs:\n",
        "    return loss, accuracy, labels, predictions\n",
        "  else:\n",
        "    return loss, accuracy, labels, predictions, probabilities\n",
        "\n",
        "\n",
        "def get_metrics(actual_values, predicted_values, index):\n",
        "  #casting to make sure everything is alright\n",
        "  actual_values = actual_values.type(torch.LongTensor).numpy()\n",
        "  predicted_values = predicted_values.type(torch.LongTensor).numpy()\n",
        "\n",
        "  gt_values = np.ma.masked_equal(actual_values, index).mask\n",
        "  mp_values = np.ma.masked_equal(predicted_values, index).mask\n",
        "\n",
        "  precision = sum(mp_values & gt_values)/sum(mp_values)\n",
        "  recall = sum(mp_values & gt_values)/sum(gt_values)\n",
        "  F1 = 2*((precision*recall)/(precision+recall))\n",
        "\n",
        "  return precision, recall, F1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBAQkavzQYFW"
      },
      "source": [
        "#loading datasets\n",
        "testing_train_dloader = DataLoader(fp_dataset_train, shuffle=False, batch_size=16, collate_fn=collate_fn)\n",
        "testing_test_dloader = DataLoader(fp_dataset_test, shuffle=False, batch_size=16, collate_fn=collate_fn)\n",
        "testing_val_dloader = DataLoader(fp_dataset_val, shuffle=False, batch_size=16, collate_fn=collate_fn)\n",
        "\n",
        "#loading checkpoints\n",
        "checkpoint_3_epochs = torch.load('./distilbert-reddit-fp-allagree/Distilbert_reddit_financial_phrasebank_3_epochs.pt', map_location=device)\n",
        "checkpoint_5_epochs = torch.load('./distilbert-reddit-fp-allagree/Distilbert_reddit_financial_phrasebank_5_epochs.pt', map_location=device)\n",
        "checkpoint_7_epochs = torch.load('./distilbert-reddit-fp-allagree/Distilbert_reddit_financial_phrasebank_7_epochs.pt', map_location=device)\n",
        "checkpoint_10_epochs = torch.load('./distilbert-reddit-fp-allagree/Distilbert_reddit_financial_phrasebank_10_epochs.pt', map_location=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vv_P5LZ9Ir2a",
        "outputId": "ffc334de-96a6-4123-923d-b70a7521cdf9"
      },
      "source": [
        "##evaluating '3_epochs'\n",
        "model = DistilBertClassifier(out_dim=3)\n",
        "model.load_state_dict(checkpoint_3_epochs['model_state_dict'])\n",
        "\n",
        "print('\\n-----train')\n",
        "train_loss, train_acc, train_true_values, train_preds = testing(model, testing_train_dloader)\n",
        "print('\\n-----test')\n",
        "test_loss, test_acc, test_true_values, test_preds = testing(model, testing_test_dloader)\n",
        "print('\\n-----val')\n",
        "val_loss, val_acc, val_true_values, val_preds = testing(model, testing_val_dloader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-----train\n",
            "---- Total accuracy: 0.9217316513761468, Total loss: 0.2329277686364607\n",
            "\n",
            "-----test\n",
            "---- Total accuracy: 0.8575819672131147, Total loss: 0.34995327657852016\n",
            "\n",
            "-----val\n",
            "---- Total accuracy: 0.8275, Total loss: 0.4211508938670158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BuRiwaLp89Za",
        "outputId": "302c9995-44f2-4f8f-d824-244d630e11c1"
      },
      "source": [
        "##evaluating weights '5_epochs'\n",
        "model = DistilBertClassifier(out_dim=3)\n",
        "model.load_state_dict(checkpoint_5_epochs['model_state_dict'])\n",
        "\n",
        "print('\\n-----train')\n",
        "train_loss_5, train_acc_5, train_true_values_5, train_preds_5 = testing(model, testing_train_dloader)\n",
        "print('\\n-----test')\n",
        "test_loss_5, test_acc_5, test_true_values_5, test_preds_5 = testing(model, testing_test_dloader)\n",
        "print('\\n-----val')\n",
        "val_loss_5, val_acc_5, val_true_values_5, val_preds_5 = testing(model, testing_val_dloader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-----train\n",
            "---- Total accuracy: 0.96875, Total loss: 0.11476771865005887\n",
            "\n",
            "-----test\n",
            "---- Total accuracy: 0.8620901635435761, Total loss: 0.3613927063883328\n",
            "\n",
            "-----val\n",
            "---- Total accuracy: 0.8225, Total loss: 0.4521456964313984\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80Evb4XXXMNo",
        "outputId": "6243e49b-961d-40e5-c00a-330fa761ae15"
      },
      "source": [
        "##evaluating weights '7_epochs'\n",
        "model = DistilBertClassifier(out_dim=3)\n",
        "model.load_state_dict(checkpoint_7_epochs['model_state_dict'])\n",
        "\n",
        "print('\\n-----train')\n",
        "train_loss_7, train_acc_7, train_true_values_7, train_preds_7 = testing(model, testing_train_dloader)\n",
        "print('\\n-----test')\n",
        "test_loss_7, test_acc_7, test_true_values_7, test_preds_7 = testing(model, testing_test_dloader)\n",
        "print('\\n-----val')\n",
        "val_loss_7, val_acc_7, val_true_values_7, val_preds_7 = testing(model, testing_val_dloader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-----train\n",
            "---- Total accuracy: 0.9899655963302753, Total loss: 0.04829801462266013\n",
            "\n",
            "-----test\n",
            "---- Total accuracy: 0.869262294691117, Total loss: 0.4218116987374474\n",
            "\n",
            "-----val\n",
            "---- Total accuracy: 0.8325, Total loss: 0.538484151288867\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJuPz_I5XOap",
        "outputId": "8b6f4692-7efc-4865-fca5-1f8bc06b1267"
      },
      "source": [
        "##evaluating weights '10_epochs'\n",
        "model = DistilBertClassifier(out_dim=3)\n",
        "model.load_state_dict(checkpoint_10_epochs['model_state_dict'])\n",
        "\n",
        "print('\\n-----train')\n",
        "train_loss_10, train_acc_10, train_true_values_10, train_preds_10 = testing(model, testing_train_dloader)\n",
        "print('\\n-----test')\n",
        "test_loss_10, test_acc_10, test_true_values_10, test_preds_10 = testing(model, testing_test_dloader)\n",
        "print('\\n-----val')\n",
        "val_loss_10, val_acc_10, val_true_values_10, val_preds_10 = testing(model, testing_val_dloader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-----train\n",
            "---- Total accuracy: 0.9954128440366973, Total loss: 0.02407797688293621\n",
            "\n",
            "-----test\n",
            "---- Total accuracy: 0.8631147537075106, Total loss: 0.4852838981896639\n",
            "\n",
            "-----val\n",
            "---- Total accuracy: 0.83, Total loss: 0.6317341728135943\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3M-uDFkxeF_"
      },
      "source": [
        "testing_df = pd.DataFrame(columns=['train', 'test', 'evaluation'])\n",
        "testing_df.loc['3_epochs'] = [train_acc, test_acc, val_acc]\n",
        "testing_df.loc['5_epochs'] = [train_acc_5, test_acc_5, val_acc_5]\n",
        "testing_df.loc['7_epochs'] = [train_acc_7, test_acc_7, val_acc_7]\n",
        "testing_df.loc['10_epochs'] = [train_acc_10, test_acc_10, val_acc_10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbUOU2lAxg-K",
        "outputId": "f410fe5c-ea0e-4b32-8958-b9c8fb77b02c"
      },
      "source": [
        "#may need to double test on glue\n",
        "testing_df.round(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>train</th>\n",
              "      <th>test</th>\n",
              "      <th>evaluation</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3_epochs</th>\n",
              "      <td>0.92</td>\n",
              "      <td>0.86</td>\n",
              "      <td>0.83</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5_epochs</th>\n",
              "      <td>0.97</td>\n",
              "      <td>0.86</td>\n",
              "      <td>0.82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7_epochs</th>\n",
              "      <td>0.99</td>\n",
              "      <td>0.87</td>\n",
              "      <td>0.83</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10_epochs</th>\n",
              "      <td>1.00</td>\n",
              "      <td>0.86</td>\n",
              "      <td>0.83</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           train  test  evaluation\n",
              "3_epochs    0.92  0.86        0.83\n",
              "5_epochs    0.97  0.86        0.82\n",
              "7_epochs    0.99  0.87        0.83\n",
              "10_epochs   1.00  0.86        0.83"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJ3zYgxF9Dyy"
      },
      "source": [
        "\n",
        "###Sentiment140"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_TTkELk5h2z"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HpDFrEh4BoB"
      },
      "source": [
        "os.environ['KAGGLE_CONFIG_DIR'] = './'\n",
        "!kaggle datasets download -d muniozdaniel0/sentiment140-tokenized\n",
        "!unzip ./sentiment140-tokenized.zip -d ./sentiment140tokenized-train\n",
        "!rm ./sentiment140-tokenized.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3mzSEZlvx6g"
      },
      "source": [
        "def training_distilbert(model, data_loader, device, test_dloader=None, model_optimizer='AdamW', learning_rate=1e-5, loss_function=nn.CrossEntropyLoss(), \n",
        "                        epochs=3, scheduler='linear', warmup_steps=600,\n",
        "                        saving_dir='./', model_name='distilber_model'):\n",
        "  model.train()\n",
        "  loss_fn = loss_function\n",
        "  optim = AdamW(model.parameters(), lr=learning_rate) if model_optimizer == 'AdamW' else model_optimizer(model.parameters(), lr=learning_rate)\n",
        "\n",
        "  num_epochs = epochs\n",
        "  num_training_steps = num_epochs * len(data_loader)\n",
        "  lr_scheduler = get_scheduler(scheduler, optimizer=optim, num_warmup_steps=warmup_steps, num_training_steps=num_training_steps)\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    epoch_loss = 0.0\n",
        "    running_loss = 0.0    \n",
        "    for i, batch in enumerate(data_loader):\n",
        "      input_ids, attention_mask, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['labels'].type(torch.LongTensor).to(device)\n",
        "      optim.zero_grad()\n",
        "\n",
        "      out = model(input_ids, attention_mask, return_hidden_embeddings=False)\n",
        "      loss = loss_fn(out, labels)\n",
        "\n",
        "      loss.backward()\n",
        "      optim.step()\n",
        "      lr_scheduler.step()\n",
        "\n",
        "      epoch_loss+=loss.item()\n",
        "      running_loss+=loss.item()\n",
        "      #may wanna add a 'limit' variable\n",
        "      if i!=0 and i%20000 == 0:\n",
        "        running_loss/=20000\n",
        "        print('epoch: {}, iter: {}, running_loss: {}   --------saving model'.format(epoch, i, running_loss))\n",
        "\n",
        "        torch.save({'epoch': epoch,\n",
        "                    'model_state_dict':model.state_dict(),\n",
        "                    'optimizer_state_dict':optim.state_dict(),\n",
        "                    'lr_scheduler':lr_scheduler.state_dict(),\n",
        "                    'running_loss':running_loss},\n",
        "                    os.path.join(saving_dir, model_name)+'_epoch_{}_iter_{}.pt'.format(epoch, i))\n",
        "        running_loss = 0.0\n",
        "        \n",
        "      #if i!=0 and i%1000==0:\n",
        "      #  print('epoch: {} iter: {} last_batch_loss: {}'.format(epoch, i, loss.item()))\n",
        "\n",
        "    epoch_loss = epoch_loss/len(data_loader)\n",
        "    print('storing \"end of the epoch\" weights epoch_loss: {}'.format(epoch_loss))\n",
        "    torch.save({'epoch': epoch,\n",
        "                'model_state_dict':model.state_dict(),\n",
        "                'optimizer_state_dict':optim.state_dict(),\n",
        "                'lr_scheduler':lr_scheduler.state_dict(),\n",
        "                'epoch_loss':epoch_loss},\n",
        "                os.path.join(saving_dir, model_name)+'_epoch_{}.pt'.format(epoch))\n",
        "     \n",
        "    if test_dloader:\n",
        "      testing_loss, testing_accuracy = inner_testing(model, test_dloader, device, keep_training=True)\n",
        "      print('----Testing:  loss: {}, accuracy: {}'.format(testing_loss, testing_accuracy))\n",
        "\n",
        "  train_loss, train_accuracy = inner_testing(model, data_loader, device)\n",
        "  print('----After training metrics:  loss: {}, accuracy: {}'.format(train_loss, train_accuracy))\n",
        "  return train_loss, train_accuracy\n",
        "\n",
        "\n",
        "def inner_testing(model, data_loader, device, keep_training=True):\n",
        "  loss = 0.0\n",
        "  accuracy = 0.0\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for _, batch in enumerate(data_loader):\n",
        "      input_ids, attention_mask, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['labels'].type(torch.LongTensor).to(device)\n",
        "      out_model = model(input_ids, attention_mask)\n",
        "      batch_loss = F.cross_entropy(out_model, labels)\n",
        "\n",
        "      probs = F.softmax(out_model, dim=1)\n",
        "      vals, idx = probs.topk(1, dim=1)\n",
        "      acc = sum(idx.squeeze() == labels) / len(idx.squeeze())\n",
        "\n",
        "      loss+=batch_loss.item()\n",
        "      accuracy+=acc.item()\n",
        "\n",
        "  loss/=len(data_loader)\n",
        "  accuracy/=len(data_loader)\n",
        "  if keep_training:\n",
        "    model.train()\n",
        "    \n",
        "  return loss, accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kvu5cW3MUG7"
      },
      "source": [
        "#defining tokenizer and Data Collator\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased') #using ver. 4.10.0.dev0\n",
        "collate_fn = Data_collator(tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taIxQKW3L9Je"
      },
      "source": [
        "#loading data\n",
        "tokenized_sentiment140 = pd.read_csv('./sentiment140tokenized-train/Sentiment140Tokenized_Train.csv', \n",
        "                                     converters={'input_ids': str2arrray, 'attention_mask': str2arrray})\n",
        "s140_train, s140_test = train_test_split(tokenized_sentiment140, test_size=0.1, random_state=2332)\n",
        "\n",
        "train_ds = Sentiment140(s140_train)\n",
        "test_ds = Sentiment140(s140_test)\n",
        "\n",
        "train_loader = DataLoader(train_ds, shuffle=True, batch_size=16, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_ds, shuffle=True, batch_size=16, collate_fn=collate_fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywnf-C5dRyM2",
        "outputId": "0ab234a1-89d7-497d-ab1a-a375e1c447d1"
      },
      "source": [
        "#Loading model\n",
        "#model = DistilBertClassifier(backbone_weights=checkpoint['model']).to(device) #no frozen layers first\n",
        "model = DistilBertClassifier(backbone_weights=checkpoint['model'], freeze_backbone=True).to(device) #freezeing layers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "backbone weights loaded succesfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktOjIo69mz2W",
        "outputId": "8b1f8fa0-0482-47ac-e41d-81900dff3c1c"
      },
      "source": [
        "training_loss, training_accuracy = training_distilbert(model, train_loader, device, test_dloader=test_loader, epochs=3,\n",
        "                                                       saving_dir='./distilbert_reddit_sentiment140_weights_frozen', \n",
        "                                                       model_name='distilbert_reddit_sentiment140_frozen')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0, iter: 20000, running_loss: 0.5207763518571854   --------saving model\n",
            "epoch: 0, iter: 40000, running_loss: 0.48912163420692084   --------saving model\n",
            "epoch: 0, iter: 60000, running_loss: 0.4817559070628136   --------saving model\n",
            "epoch: 0, iter: 80000, running_loss: 0.4773371273621917   --------saving model\n",
            "storing \"end of the epoch\" weights epoch_loss: 0.4903511339673565\n",
            "----Testing:  loss: 0.4551193830393255, accuracy: 0.7836375\n",
            "epoch: 1, iter: 20000, running_loss: 0.47466593148186803   --------saving model\n",
            "epoch: 1, iter: 40000, running_loss: 0.4708296662375331   --------saving model\n",
            "epoch: 1, iter: 60000, running_loss: 0.4685318947955966   --------saving model\n",
            "epoch: 1, iter: 80000, running_loss: 0.4698149767607451   --------saving model\n",
            "storing \"end of the epoch\" weights epoch_loss: 0.47061067622088726\n",
            "----Testing:  loss: 0.4487661852501333, accuracy: 0.78718125\n",
            "epoch: 2, iter: 20000, running_loss: 0.4658794046401978   --------saving model\n",
            "epoch: 2, iter: 40000, running_loss: 0.4659000628300011   --------saving model\n",
            "epoch: 2, iter: 60000, running_loss: 0.46707539846375584   --------saving model\n",
            "epoch: 2, iter: 80000, running_loss: 0.4643298405256122   --------saving model\n",
            "storing \"end of the epoch\" weights epoch_loss: 0.4657545945646862\n",
            "----Testing:  loss: 0.44414490223899483, accuracy: 0.78995\n",
            "----After training metrics:  loss: 0.44328316023556724, accuracy: 0.7914486111111111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zp7iQ8087bCZ"
      },
      "source": [
        "storing \"end of the epoch\" weights epoch_loss: 0.4903511339673565\n",
        "----Testing:  loss: 0.4551193830393255, accuracy: 0.7836375\n",
        "storing \"end of the epoch\" weights epoch_loss: 0.47061067622088726\n",
        "----Testing:  loss: 0.4487661852501333, accuracy: 0.78718125\n",
        "storing \"end of the epoch\" weights epoch_loss: 0.4657545945646862\n",
        "----Testing:  loss: 0.44414490223899483, accuracy: 0.78995\n",
        "\n",
        "----After training metrics:  loss: 0.44328316023556724, accuracy: 0.7914486111111111"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UoPAaj-eURI"
      },
      "source": [
        "###Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n90UVIFIB9Ay"
      },
      "source": [
        "def get_metrics(actual_values, predicted_values, index):\n",
        "  #casting datatypes to make sure everything is alright\n",
        "  actual_values = actual_values.type(torch.LongTensor).numpy()\n",
        "  predicted_values = predicted_values.type(torch.LongTensor).numpy()\n",
        "\n",
        "  gt_values = np.ma.masked_equal(actual_values, index).mask\n",
        "  mp_values = np.ma.masked_equal(predicted_values, index).mask\n",
        "\n",
        "  precision = sum(mp_values & gt_values)/sum(mp_values)\n",
        "  recall = sum(mp_values & gt_values)/sum(gt_values)\n",
        "  F1 = 2*((precision*recall)/(precision+recall))\n",
        "\n",
        "  return precision, recall, F1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKhPX5cwQUmx"
      },
      "source": [
        "def testing_classifier(model, data_loader, device, keep_training=True):\n",
        "  loss = 0.0\n",
        "  accuracy = 0.0\n",
        "\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for _, batch in enumerate(data_loader):\n",
        "      input_ids, attention_mask, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['labels'].type(torch.LongTensor).to(device)\n",
        "      out_model = model(input_ids, attention_mask)\n",
        "      batch_loss = F.cross_entropy(out_model, labels)\n",
        "\n",
        "      probs = F.softmax(out_model, dim=1)\n",
        "      vals, idx = probs.topk(1, dim=1)\n",
        "      acc = sum(idx.squeeze() == labels) / len(idx.squeeze())\n",
        "\n",
        "      loss+=batch_loss.item()\n",
        "      accuracy+=acc.item()\n",
        "\n",
        "  loss/=len(data_loader)\n",
        "  accuracy/=len(data_loader)\n",
        "  if keep_training:\n",
        "    model.train()\n",
        "    \n",
        "  return loss, accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEw1KHkGB_r3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}