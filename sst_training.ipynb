{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sst_training.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "DVH3kjpBzKMR",
        "NpYMwvQMpPZ4",
        "qRzrT2f_IwWk",
        "K995P33epYaF"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rmr-dcyX1GrG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebc3433e-32c8-4b64-f05c-252ad1d6977e"
      },
      "source": [
        "!pip install datasets\n",
        "!pip install transformers"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-1.15.1-py3-none-any.whl (290 kB)\n",
            "\u001b[K     |████████████████████████████████| 290 kB 4.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 53.9 MB/s \n",
            "\u001b[?25hCollecting aiohttp\n",
            "  Downloading aiohttp-3.8.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 39.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.2)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Collecting huggingface-hub<1.0.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.1.2-py3-none-any.whl (59 kB)\n",
            "\u001b[K     |████████████████████████████████| 59 kB 6.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.62.3)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Collecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2021.11.0-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 74.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.8.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing<3,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.2.0)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.1-py3-none-any.whl (5.7 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (192 kB)\n",
            "\u001b[K     |████████████████████████████████| 192 kB 61.5 MB/s \n",
            "\u001b[?25hCollecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n",
            "\u001b[K     |████████████████████████████████| 160 kB 49.0 MB/s \n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 59.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, fsspec, aiohttp, xxhash, huggingface-hub, datasets\n",
            "Successfully installed aiohttp-3.8.0 aiosignal-1.2.0 async-timeout-4.0.1 asynctest-0.13.0 datasets-1.15.1 frozenlist-1.2.0 fsspec-2021.11.0 huggingface-hub-0.1.2 multidict-5.2.0 xxhash-2.0.2 yarl-1.7.2\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.12.3-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 45.5 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 69.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.1.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 38.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing<3,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.12.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVktkUt4en7q"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from collections import OrderedDict\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from datasets import load_dataset\n",
        "from transformers import AdamW, get_scheduler, DistilBertModel, DistilBertConfig, DistilBertTokenizerFast\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAYUJwp62HBU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99a683f3-0796-4ec5-cdd2-07c4d19b24d8"
      },
      "source": [
        "#loads only backbone weights\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = '/content'\n",
        "!kaggle datasets download -d muniozdaniel0/final-distilbert-weight\n",
        "!unzip ./final-distilbert-weight.zip -d ./final-distilbert-weight\n",
        "!rm ./final-distilbert-weight.zip"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /content/kaggle.json'\n",
            "Downloading final-distilbert-weight.zip to /content\n",
            "100% 702M/705M [00:05<00:00, 131MB/s]\n",
            "100% 705M/705M [00:05<00:00, 131MB/s]\n",
            "Archive:  ./final-distilbert-weight.zip\n",
            "  inflating: ./final-distilbert-weight/distilbert_reddit_epoch_2_iter_final.pt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVhHrpogGR4k"
      },
      "source": [
        "#loading backbone checkpoint ----may be a good idea to change it to 'backbone_chackpoint'\n",
        "checkpoint = torch.load('./final-distilbert-weight/distilbert_reddit_epoch_2_iter_final.pt', map_location=device)\n",
        "checkpoint['model'] = OrderedDict({k[11:]: v for k, v in checkpoint['model'].items() if 'distilbert' in k})  #OrderedDict to keep it consistant with pytorch"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVH3kjpBzKMR"
      },
      "source": [
        "###Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "io0GxzxAzXnc"
      },
      "source": [
        "class DistilBertRegressor(nn.Module):\n",
        "  def __init__(self, backbone_weights=None, freeze_backbone=False):\n",
        "    super().__init__()  \n",
        "    self.backbone = DistilBertModel(config=DistilBertConfig())\n",
        "    \n",
        "    if backbone_weights:\n",
        "      self.backbone.load_state_dict(backbone_weights)\n",
        "      print('backbone weights loaded succesfully')\n",
        "    if freeze_backbone:\n",
        "      for param in self.backbone.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    self.linear =  nn.Linear(self.backbone.config.dim, self.backbone.config.dim)  #'backbone.config.dim' stores the output dimension of the backbone\n",
        "    self.dropout = nn.Dropout(0.2)\n",
        "    self.regressor = nn.Linear(self.backbone.config.dim, 1)\n",
        "  \n",
        "  #Could be useful to meassure differences? one model using the first layer of a pretrained classifier vs one being randomly initialized.\n",
        "  def forward(self, input_ids, attention_mask, return_hidden_embeddings=False):\n",
        "    hidden_embeddings = self.backbone(input_ids=input_ids, attention_mask=attention_mask)[0]\n",
        "    out = self.linear(hidden_embeddings[:, 0, :])\n",
        "    out = F.relu(out)\n",
        "    out = self.dropout(out)\n",
        "    out = self.regressor(out)\n",
        "\n",
        "    if not return_hidden_embeddings:\n",
        "      return out\n",
        "    else:\n",
        "      return hidden_embeddings, out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aywdpWczL-G"
      },
      "source": [
        "#### ---Current Classifier Version---\n",
        "class DistilBertClassifier(nn.Module):\n",
        "  def __init__(self, out_dim=2, backbone_weights=None, freeze_backbone=False):\n",
        "    super().__init__()  \n",
        "    self.backbone = DistilBertModel(config=DistilBertConfig())\n",
        "    \n",
        "    if backbone_weights:\n",
        "      self.backbone.load_state_dict(backbone_weights)\n",
        "      print('backbone weights loaded succesfully')\n",
        "    if freeze_backbone:\n",
        "      for param in self.backbone.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    self.linear =  nn.Linear(self.backbone.config.dim, self.backbone.config.dim)  #'backbone.config.dim' stores the output dimension of the backbone\n",
        "    self.dropout = nn.Dropout(0.2)\n",
        "    self.classifier = nn.Linear(self.backbone.config.dim, out_dim)\n",
        "  \n",
        "  def forward(self, input_ids, attention_mask, return_hidden_embeddings=False):\n",
        "    hidden_embeddings = self.backbone(input_ids=input_ids, attention_mask=attention_mask)[0]\n",
        "    out = self.linear(hidden_embeddings[:, 0, :])\n",
        "    out = F.relu(out)\n",
        "    out = self.dropout(out)\n",
        "    out = self.classifier(out)\n",
        "\n",
        "    if not return_hidden_embeddings:\n",
        "      return out\n",
        "    else:\n",
        "      return hidden_embeddings, out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpYMwvQMpPZ4"
      },
      "source": [
        "###sst-2 Regressor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lUMJii8mAVl"
      },
      "source": [
        "#sst-2 tokenizer_fn and collator\n",
        "def tokenizer_map(element):\n",
        "  return tokenizer(element['sentence'], max_length=tokenizer.model_max_length, truncation=True)\n",
        "\n",
        "class Data_collator():\n",
        "  def __init__(self, tokenizer): \n",
        "    self.tokenizer = tokenizer\n",
        "\n",
        "  def __call__(self, batch):\n",
        "    #Assumes we are wotking with a list of dictionaries\n",
        "    training_dict = {'input_ids': [element['input_ids'] for element in batch], \n",
        "                     'attention_mask': [element['attention_mask'] for element in batch]}\n",
        "    training_samples = self.tokenizer.pad(training_dict, return_tensors = 'pt')\n",
        "    labels = torch.tensor([element['label'] for element in batch], dtype=torch.float32)\n",
        "\n",
        "    return {'input_ids': training_samples['input_ids'],\n",
        "            'attention_mask': training_samples['attention_mask'],\n",
        "            'labels': labels}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJm6DBx1l1b-"
      },
      "source": [
        "#defining tokenizer and DataCollator\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')                    #using ver. 4.10.0.dev0\n",
        "collate_fn = Data_collator(tokenizer)\n",
        "\n",
        "#load sst-2 dataset\n",
        "sst_dataset = load_dataset(\"sst\", \"default\")\n",
        "\n",
        "tokenized_sst_dataset = sst_dataset.map(tokenizer_map, remove_columns=['sentence', 'tokens', 'tree'], batched=True)\n",
        "sst_train_loader = DataLoader(tokenized_sst_dataset['train'], shuffle=True, batch_size=16, collate_fn=collate_fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cncvOH5s1zxg"
      },
      "source": [
        "#Initializing model\n",
        "#model = DistilBertRegressor(backbone_weights=checkpoint['model']).to(device)\n",
        "model = DistilBertRegressor(backbone_weights=checkpoint['model'], freeze_backbone=True).to(device)\n",
        "\n",
        "#defining loss and optimizer\n",
        "optim = AdamW(model.parameters(), lr=1e-5)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "num_epochs = 3\n",
        "\n",
        "num_training_steps = num_epochs * len(sst_train_loader)\n",
        "lr_scheduler = get_scheduler('linear', optimizer=optim, num_warmup_steps=600, num_training_steps=num_training_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1E7b__Q2IGiC"
      },
      "source": [
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "  for i, batch in enumerate(sst_train_loader):\n",
        "    input_ids, attention_mask, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['labels'].to(device)\n",
        "    optim.zero_grad()\n",
        "\n",
        "    out = model(input_ids, attention_mask, return_hidden_embeddings=False)\n",
        "    \n",
        "    #we may need to average the loss over the whole epoch\n",
        "    loss = loss_fn(out.squeeze(), labels)\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "    lr_scheduler.step()\n",
        "    if i!=0 and i%200 == 0:\n",
        "      print('saving model at iter{}'.format(i))\n",
        "      \n",
        "      torch.save({'epoch': epoch,\n",
        "                  'model_state_dict':model.state_dict(),\n",
        "                  'optimizer_state_dict':optim.state_dict(),\n",
        "                  'lr_scheduler':lr_scheduler.state_dict(),\n",
        "                  'local_loss':loss.item()},\n",
        "                  './Distilbert_reddit_sst_weights_frozenbackbone/Distilbert_reddit_sst_epoch_{}_iter_{}.pt'.format(epoch, i))\n",
        "    if i!=0 and i%100==0:\n",
        "      print('epoch: {} iter: {} loss: {}'.format(epoch, i, loss.item()))\n",
        "\n",
        "  print('storing \"end of the epoch\" weights')\n",
        "  torch.save({'epoch': epoch,\n",
        "              'model_state_dict':model.state_dict(),\n",
        "              'optimizer_state_dict':optim.state_dict(),\n",
        "              'lr_scheduler':lr_scheduler.state_dict(),\n",
        "              'local_loss':loss.item()},\n",
        "              './Distilbert_reddit_sst_weights_frozenbackbone/Distilbert_reddit_sst_epoch_{}.pt'.format(epoch))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRzrT2f_IwWk"
      },
      "source": [
        "####Regressor Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsxMy9cP8o7w"
      },
      "source": [
        "#loading weights\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = '/content'\n",
        "!kaggle datasets download -d muniozdaniel0/distilbert-reddit-sst-weights-frozenbackbone-2\n",
        "!unzip ./distilbert-reddit-sst-weights-frozenbackbone-2.zip -d ./distilbert-reddit-sst-weights-frozenbackbone-2\n",
        "!rm ./distilbert-reddit-sst-weights-frozenbackbone-2.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCJEPBflhPm4"
      },
      "source": [
        "#regressor values test\n",
        "#loading sst_frozen checkpoint\n",
        "sst_frozen_checkpoint = torch.load('./distilbert-reddit-sst-weights-frozenbackbone-2/Distilbert_reddit_sst_epoch_2.pt', map_location=device)\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')                    #using ver. 4.10.0.dev0\n",
        "\n",
        "#loading model\n",
        "model = DistilBertRegressor().to(device)\n",
        "model.load_state_dict(sst_frozen_checkpoint['model_state_dict'])\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRBdhD69j1ox"
      },
      "source": [
        "#load dataset\n",
        "sst_dataset = load_dataset(\"sst\", \"default\")\n",
        "tokenized_sst_dataset = sst_dataset.map(tokenizer_map, remove_columns=['sentence', 'tokens', 'tree'], batched=True)\n",
        "\n",
        "collate_fn = Data_collator(tokenizer)\n",
        "sst_test_loader = DataLoader(tokenized_sst_dataset['test'], shuffle=False, batch_size=16, collate_fn=collate_fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlKyItDGrkOt"
      },
      "source": [
        "with torch.no_grad():\n",
        "  cumulative_loss = 0.0\n",
        "  out_values = torch.tensor([])\n",
        "  labels_values = torch.Tensor([])\n",
        "  for _, batch in enumerate(sst_test_loader):\n",
        "    out = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n",
        "    local_loss = nn.MSELoss()(out.view(-1), batch['labels'])\n",
        "  \n",
        "    cumulative_loss+=local_loss\n",
        "    out_values = torch.cat((out_values, out.view(-1)))\n",
        "    labels_values = torch.cat((labels_values, batch['labels']))\n",
        "\n",
        "    print(local_loss, out.shape)\n",
        "  global_eval_loss = cumulative_loss/len(sst_test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwKN3-YI_Sji"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "dummy_y = np.zeros_like(out_values.numpy())\n",
        "out_values.min(), out_values.max()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431
        },
        "id": "je6Y3EKaJPup",
        "outputId": "a9550eae-fb44-4f86-fb13-b2cc062ceebc"
      },
      "source": [
        "figure(figsize=(24, 6), dpi=80)\n",
        "plt.scatter(labels_values.numpy(), dummy_y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7f8554d5a7d0>"
            ]
          },
          "metadata": {},
          "execution_count": 156
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABgkAAAGMCAYAAADgLkKMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAMTQAADE0B0s6tTgAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de7hdZX0v+u+bC5QQDLdwXQkBCXdBEAVvtNZaa+um7tqnSsuxpR5FT7W2Wiu23duzT1sv7Slnd7d9FHVX66a7unu01UrVehSkoqg0QUBuARpgcQ1XCaGay3v+WCvpWMlca17WSLLC+HyeJ8+TMX9jvesda4455pjvd47xllprAAAAAACA7pm3uzsAAAAAAADsHkICAAAAAADoKCEBAAAAAAB0lJAAAAAAAAA6SkgAAAAAAAAdJSQAAAAAAICOEhIAAAAAAEBHLdjdHWjL3nvvXZcuXbq7uwEAAAAAAHPKPffc88Na6969ak+bkGDp0qUZHx/f3d0AAAAAAIA5pZSybrqa2w0BAAAAAEBHCQkAAAAAAKCjhAQAAAAAANBRQgIAAAAAAOgoIQEAAAAAAHSUkAAAAAAAADpKSAAAAAAAAB0lJAAAAAAAgI4SEgAAAAAAQEcJCQAAAAAAoKOEBAAAAAAA0FFCAgAAAAAA6CghAQAAAAAAdJSQAAAAAAAAOkpIAAAAAAAAHSUkAAAAAACAjhISAAAAAABARwkJAAAAAACgo4QEAAAAAADQUUICAAAAAADoKCEBAAAAAAB0lJAAAAAAAAA6SkgAAAAAAAAdJSQAAAAAAICOEhIAAAAAAEBHCQkAAAAAAKCjhAQAAAAAANBRQgIAAAAAAOgoIQEAAAAAAHSUkAAAAAAAADpKSAAAAAAAAB0lJAAAAAAAgI4SEgAAAAAAQEcJCQAAAAAAoKOEBAAAAAAA0FFCAgAAAAAA6CghAQAAAAAAdJSQAAAAAAAAOkpIAAAAAAAAHSUkAAAAAACAjhISAAAAAABARwkJAAAAAACgo4QEAAAAAADQUa2GBKWUlaWUb5RSbi2lfKeUcvI0672+lLKmlHJ7KeUjpZSF29VLKeWrpZTH2uwfAAAAAADw79q+kuCSJB+utR6X5ANJPr79CqWUo5P8fpIXJzk2yaFJ3rjdar+Z5PaW+wYAAAAAADS0FhKUUg5JcmaSSycf+nSSZaWUY7db9eeTfK7Wen+ttSb5UJLzGu2cnORVSd7fVt8AAAAAAIAdtXklwbIk99VaNyXJZABwV5Ll2623PMmdjeW1W9eZvO3QR5JcmGRzi30DAAAAAAC2M9cmLn5Pks/UWm/qt2Ip5e2llPGt/9avX78LugcAAAAAAE8fbYYEdyc5vJSyIJmYfDgTVwjctd16dyU5qrG8orHOjyZ5ayllbZKvJ3lGKWVtKWXp9r+s1npxrXVs67/Fixe3uCkAAAAAAPD011pIUGt9MMmqJOdPPvTqJOO11tu2W/XTSc4tpRw2GSS8KcknJ9t4ca31qFrriiQvSvL9WuuKWuu6tvoJAAAAAABMaPt2QxcmubCUcmuSi5JckCSllI+WUs5NklrrHZm4rdBVSW5Lsi7JJS33AwAAAAAA6KNMzC+85xsbG6vj4+O7uxsAAAAAADCnlFLuqbWO9arNtYmLAQAAAACAXURIAAAAAAAAHSUkAAAAAACAjhISAAAAAABARwkJAAAAAACgo4QEAAAAAADQUUICAAAAAADoKCEBAAAAAAB0lJAAAAAAAAA6SkgAAAAAAAAdJSQAAAAAAICOEhIAAAAAAEBHCQkAAAAAAKCjhAQAAAAAANBRQgIAAAAAAOgoIQEAAAAAAHSUkAAAAAAAADpKSAAAAAAAAB0lJAAAAAAAgI4SEgAAAAAAQEcJCQAAAAAAoKOEBAAAAAAA0FFCAgAAAAAA6CghAQAAAAAAdJSQAAAAAAAAOkpIAAAAAAAAHSUkAAAAAACAjhISAAAAAABARwkJAAAAAACgo4QEAAAAAADQUUICAAAAAADoKCEBAAAAAAB0lJAAAAAAAAA6SkgAAAAAAAAdJSQAAAAAAICOEhIAAAAAAEBHCQkAAAAAAKCjhAQAAAAAANBRQgIAAAAAAOgoIQEAAAAAAHSUkAAAAAAAADpKSAAAAAAAAB0lJAAAAAAAgI4SEgAAAAAAQEcJCQAAAAAAoKOEBAAAAAAA0FFCAgAAAAAA6CghAQAAAAAAdJSQAAAAAAAAOkpIAAAAAAAAHSUkAAAAAACAjhISAAAAAABARwkJAAAAAACgo4QEAAAAAADQUUICAAAAAADoKCEBAAAAAAB0lJAAAAAAAAA6qtWQoJSyspTyjVLKraWU75RSTp5mvdeXUtaUUm4vpXyklLJw8vEfL6V8u5RyYynle6WUPyqlCDIAAAAAAGAnaHsA/pIkH661HpfkA0k+vv0KpZSjk/x+khcnOTbJoUneOFl+NMlra60nJXlOkhckeV3LfQQAAAAAANJiSFBKOSTJmUkunXzo00mWlVKO3W7Vn0/yuVrr/bXWmuRDSc5Lklrr6lrrHZP//7ck1yZZ0VYfAQAAAACAf9fmlQTLktxXa92UJJMBwF1Jlm+33vIkdzaW1/ZYJ6WUwzIRKHy+xT4CAAAAAACT5uT9/kspz0jyD0n+qNZ6zTTrvL2UMr713/r163dtJwEAAAAAYA/XZkhwd5LDSykLkqSUUjJxhcBd2613V5KjGssrmuuUUvZL8sUkn621XjzdL6u1XlxrHdv6b/Hixe1sBQAAAAAAdERrIUGt9cEkq5KcP/nQq5OM11pv227VTyc5t5Ry2GSQ8KYkn0ySUsriTAQEX6y1/kFbfQMAAAAAAHbU9u2GLkxyYSnl1iQXJbkgSUopHy2lnJskkxMTvyfJVUluS7IuySWTP/+2JM9L8nOllGsn//1uy30EAAAAAACSlIn5hfd8Y2NjdXx8fHd3AwAAAAAA5pRSyj211rFetTk5cTEAAAAAALDzCQkAAAAAAKCjhAQAAAAAANBRQgIAAAAAAOgoIQEAAAAAAHSUkAAAAAAAADpKSAAAAAAAAB0lJAAAAAAAgI4SEgAAAAAAQEcJCQAAAAAAoKOEBAAAAAAA0FFCAgAAAAAA6CghAQAAAAAAdJSQAAAAAAAAOkpIAAAAAAAAHSUkAAAAAACAjhISAAAAAABARwkJAAAAAACgo4QEAAAAAADQUUICAAAAAADoKCEBAAAAAAB0lJAAAAAAAAA6SkgAAAAAAAAdJSQAAAAAAICOEhIAAAAAAEBHCQkAAAAAAKCjhAQAAAAAANBRQgIAAAAAAOgoIQEAAAAAAHSUkAAAAAAAADpKSAAAAAAAAB0lJAAAAAAAgI4SEgAAAAAAQEcJCQAAAAAAoKOEBAAAAAAA0FFCAgAAAAAA6CghAQAAAAAAdJSQAAAAAAAAOkpIAAAAAAAAHSUkAAAAAACAjhISAAAAAABARwkJAAAAAACgo4QEAAAAAADQUUICAAAAAADoKCEBAAAAAAB0lJAAAAAAAAA6SkgAAAAAAAAdJSQAAAAAAICOEhIAAAAAAEBHCQkAAAAAAKCjhAQAAAAAANBRQgIAAAAAAOgoIQEAAAAAAHSUkAAAAAAAADpKSAAAAAAAAB0lJAAAAAAAgI4SEgAAAAAAQEctaLOxUsrKJH+V5OAkjyf5lVrr93qs9/okF2UipPhqkv+j1rqxX43+Nm/enPd+4eZcN/54Th1bkt95xQmZP3/+bmsnSTZu3JjXfeyarHlgfVYeujifuODMLFy4cKS2Nm3alLd96ru58d7v56QjnpE/fc1pWbBgtN14/fr1OeN9V+aHm2v2ml+y6t3nZPHixUO38+STT+bM91+ZpzZuyT4L5+Wai87JvvvuO1KfnnjiiZz+viuzaUuyYF6y+t3nZL/99huprcceeyynv/+q1CQlyeqLXpj9999/pLY2bNiQsz5wZdb/YHMW7z0/33rXOVm0aNFIbT3yyCM544++uW151W8/PwceeODQ7axbty7P/ZNvb1v+zjuel6VLl47Up3vvvTcv+G+rty1/49dPzxFHHDFSW1dffXVe+/cPb1v+5KsOytlnnz1SW2vXrs2PfejfD6FXvOnkrFixYqS2Vq9enf/4qXu3Lf/da47I6aefPnQ7t9xyS17+sdu2LX/pgmNz/PHHj9Snm2++OT/18du3LX/xV56ZE044YaS2rrvuupz7P+/etvy5X1yWU089daS2rr322rzqk/dsW/771x6ZZz/72SO1dcMNN+SVl965bfnz5x+VU045Zeh2Pv/5y/KWr//78p+/KHnlK39mpD6tWrUqP/e/7tu2/JlfODxnnHHGSG3ddNNNecVf3bFt+Qu/fExOPPHE3dpWm8/f7bffnpd+5OZty195wwl55jOfudvbavO109b+cOedd+ZHP3jDtuWvvfmUHHXUUSP16cEHH8zzLv7OtuVvv/25OeSQQ3Z7W23u7+Pj43nRn3932/LX33JaxsbGhm7ngQceyFn/zzXblr/1m2fm0EMPHalPDz30UM78v7+1bfma3zorBx988EhttXXsS5JHH300p3/gG9uWV7/rBTnggAOGbqfN7Xv44YfznD++etvyv7zz7Bx00EEjtdXm+2qb53733Xdfnv+nq7Ytf/NtZ+Twww8fup02X4O9jjPLly/PNXc+mrUPPZkVB++bM486IKWUvm21tX1Jcs899+SFf3bttuWr3vrsHHnkkSO1df/99+fs//ov25av/o3n5LDDDhu6nTvuuCM//uGbti1/9Y0n5phjjhmpT4P+rWqtfZ+LNWvW5GX//dZty19+/XFZuXLlSP269dZb85N/uWbb8j/96socd9xxfX+uVz/Xrl2bl1xy47Z1Lr/wpBx99NFDb1+v1/Nxxx030j466vZt2bIll37rrtxwz+M55cglOf+s5bn55pvz05/4123r/OPrjs5JJ53Ut61errjiivzKF5/ctvyxly/K4qNPG3r7rr/++vyHv75r2/I//NLyPOtZzxqpTzfeeGNr29fm+9fll1+eC760Ydvyx16+KC95yUtGaqut187dd9+dF//FdduW//nXTs2yZctG6tOon1V77aPXX399fvZvxret89nzxnLaaaeN1K/tt/Frbz4lX7u3Tvl98+YN9n3ptsYuHn/88Zz+vq9nSyYGOle/+0VZsmTJ0O0k7Z0XJe2O+z311FN54R9fmcc2bMr+ixbkqneek3322Wfodtoc82uzra4otdb2Givlq0k+UWv9eCnl55O8q9b63O3WOTrJVUnOSPJAks8m+VKt9S9mqvX73WNjY3V8fLzfak9rX7j+vrz5r1ft8PgHf+mMvOJZg5/0ttVOkvzZV27Nn3x5zQ6Pv+NlK/PWl/Y/yWm69Jtr83uf3SFzyh/87Mk5//krhmrrZ/70ynzvvid2ePzkw/fLZW87Z+B2fu4vvp5Vdz++w+NnLFuSz/zai4bq08v+5IqsWffkDo+vXLpvvvyOHxuqrbP/8Mu5/4kf7vD4Yfvtlat/92VDtfWLH/5mvnHHIzs8/oJjDsz/fOPzh2rrjP/ypTzy1KYdHj9wnwVZ9Z6XD9zOyf/pH/Pkxh2PXfsuLPne7//0UH069qLLsmOPJhLU294/3CDsiosum7a29mnQ1lzsUxfamot9mqttzcU+daGtNvt04u9dlh5vE9lnQXLTH+y+ttrcxmMuuixbejw+L8kdQ7R1/O9elh9s3vHxvecnt/zhcH161n/+Qp744Y692m+vebn+/3rFUG21+bd67u//U9Y9ueN3hZbuuzDf+U8/OXA7bW7fae/5Yh7v8Ydfsvf8fPe//NRQbbX5t2rz3G/luy9Lj9OsLCzJmvcN3q9d9RpcOL9k4fx52bh5S5YduCif+NXnZeyA6b/M0tb2JckzL7osPV6GmZ/k9iGfw+N+57L02E2z17zk1vfunmPyoH+r8Uc35HV/+e3c/ciGaZ+LufD+1aufGzdPPx6yta3Zbt+w++io23fN2kdy3keunnGbBm1r2H4t2mv+Tt8+bc2uraMvuiy99oyS5F93UZ929j463TY2LZxf8jdvODtnrph5sL+tsYsXve//y/jjP9jh8bEle+fr7/6JgdtJ2jsvStod93v9x76dr9yybofHX3r80vz3C543cDttjvm12dbTTSnlnlprz28ntXa7oVLKIUnOTHLp5EOfTrKslHLsdqv+fJLP1VrvrxMJxYeSnDdAjRls3ry55ws8Sd7816uyeXOv09ed104ycQVBr4AgSf7ky2uycePgF4hs2rSp5ws8SX7vs9/Lpk29hnl7W79+fc+AIEm+d98TWb9+/UDtPPnkkz0DgiRZdffjefLJHQf8p/PEE0/0DAiSZM26J/PEE73728tjjz3W80Niktz/xA/z2GOPDdzWhg0begYESfKNOx7Jhg0betZ6eeSRR3q+ySbJI09tyiOP9P4921u3bl3PgCBJntxYs27djm9O07n33nt7BgRJsmmyPqirr756VvWmtWvXzqretHr16lnVt7rllltmVW+6+eabZ1Vvuu6662ZVb7r22mtnVW+64YYbZlXf6vOfn/4EfJB606pVvY/tg9abbrrpplnVd0ZbbT5/t99++6zqO6utNl87be0Pd95556zqTQ8++GDPAcUkeWrTRH13tNXm/j4+Pt4zIEiSLZP1QTzwwAM9A4Ik+cHmifqgHnrooZ4D6EnyxA+35KGHHhq4rbaOfcnEN+V6fRBOknVPbsyjjz46UDttbt/DDz/cMyBIksd/sDkPP/xwz1ovbb6vtnnud9999/UcFE6SjXWiPog2X4P9jiMbN9ds+OHmbNxcc+fDG/LLf/ntTPdFuLa2L5m4gmC6T0abJ+uDuv/++3sGBEnywy0T9UHccccds6o3Dfq3qrXmdX/57dz58IZpn4s1a3p/JtyqX73p1ltvHak+XT9n8q//+q8DbV+/1+sw++io27dly5ahBl+TiW/gD+qKK66YsT7o9l1//fUzttOv3tSv/8NsX5vvX5dffvms6k1tvXbuvvvuaQfP62R9UKN+Vh1lH/3ud7/bf6VJM21j08bNNed95Ops2TLdGVl7YxePP/54z4AgScYf/0Eef7z3eFIvbZ0XJe2O+z311FM9A4Ik+cot6/LUU08N1E6bY35tttU1bc5JsCzJfbXWTUkyOch/V5Ll2623PEnzTG9tY52ZaszgvV+YeWCgX73tdpLkdR+7Zlb1prd9auY3h371pjPed+Ws6lud+f6Z1+tXbzq9z+/sV5+y7vuvmlW96awPzPx7+9WbmpfpjVLfqnmLoVHqTc1bDI1Sb2reYmiUelPzss1R6k3NWwyNUt+qeen0KPWm5m1SRqk3NW8xNEq9qXmLmlHqTc3LlEepb9W8xdAo9abmLWVGqTc1b7kySn1ntNXm89e8LdAo9Z3VVpuvnbb2h+atP0apNzVvSTJKfWe11eb+3rzF0Cj1rZq3GBql3tS8Bc8o9aa2jn1JplxKP0p9qza3r3mLoVHqTW2+r7Z57te8rcwo9a3afA0OcxzZvKXmrkc25Jo7ew+WtLV9SabcYmiUelPzFkOj1Ldq3mJolHrToH+ra+58NOOPPJXNW6YOzzWfi+ZtUnrpV29q3oJnmPp0/ZzJSy65caDtG+b12m8fHXX7Lv3WXUMNviaZcouefpq3GJpJv+1r3mJolHpTv/4Ps31tvn81bzE0Sr2prddO8/Y7o9SbRv2sOso+2rwFUT/DbMPGzTWXfmv6fa2tsYvT3zfzh7V+9SnrtnRelLQ77vfCP555TKhffas2x/zabKtr9tiJi0spby+ljG/9N+i3v5+urhufOYHsV2+7nSRZ88DMz0m/etON935/VvWmH/Z5Y+pX3+qpjdMnz4PUmzb1WbVfvalf74d5W14/3dcUB6wDALBztXnu93SwcP68rH1o8Ct6mZ21Dz2ZBfN734N+Lj0XM/Vz1J8bdft2xt/lhnsG/5y+s82l5525Yy7to8mu6U+/YZwhhnla1ea432MbZv5Wfr/6Vm2O+bXZVte0GRLcneTwUsqCJCkTs9Usz8TVBE13JWnOZLeisc5MtSlqrRfXWse2/htlwtmnk1PHZp70pF+97XaSZOWhMz8n/epNJx3xjFnVm/bqc3LYr77VPgtnfvn0qzct6LNqv3pTv94Pc2q8eO+ZJ63pVwcAYOdq89zv6WDj5i1ZcfC+u7sbnbHi4H2zcXPvoa659FzM1M9Rf27U7dsZf5dTjhxtEtSdYS4978wdc2kfTXZNf/oN4+yub223Oe63/6KZJwLuV9+qzTG/Ntvqmtb2yVrrg0lWJTl/8qFXJxmvtW5/7d2nk5xbSjlsMkh4U5JPDlBjBr/zihNmVW+7nST5xAVnzqre9KevmXlm+371plXvnnli4n71ra65aOb1+tWbVvf5nf3qU9a96IWzqjd9610z/95+9aZVvz3zJMf96lt95x0zT3zTr970jV8/fVb1pk++6qBZ1ZuueNPJs6o3/d1rjphVfasvXbD99DLD1Zu++CvPnFW96XO/uGxW9aa/f+2Rs6o3ff78o2ZV3+rP+8x/3q/e9JlfmHkCqn71pi/88jGzqu+Mttp8/r7yhpnf6/rVd1Zbbb522tofvvbmU2ZVb/r22587q/rOaqvN/f3rb5n5fKVffatv/ebM51D96k3X/NZZs6o3tXXsS5LV73rBrOpbtbl9//LOs2dVb2rzfbXNc79vvu2MWdW3avM1OMxxZP68kuUHLsqZRx3Qs97W9iXJVW999qzqTVf/xnNmVd/qq288cVb1pkH/VmcedUCWHbgo8+dNjaOaz8WXX3/cjG31qzf906+uHKk+XT9ncvmFJw20fcO8Xvvto6Nu3/lnLc/CIa+U+MfXHT3wuh//qcEG/ftt3z/80sx3k+5Xb+rX/2G2r833r4+9fPpJmwepN7X12vnnXzt1VvWmUT+rjrKPfva8nnOr9jTMNiycX3L+WdPva22NXax+98wf1vrVp6zb0nlR0u6431XvnHlMqF99qzbH/Npsq2vaDq4uTHJhKeXWJBcluSBJSikfLaWcmyS11juSvCfJVUluS7IuySX9asxs/vz5+eAv9T6R+9D5z8n8+YN947utdpJk4cKFecfLep/E/PbLj8/ChQsHbmvBggX5g5/t/Wbz3v/4rCxYMFg6mSSLFy/OyYfv17N26pHPyKBXpey77745Y1nvhPW5R+2fffcd/NsT++23X1Yu7b3+CYcuzn779e5vL/vvv38O22+vnrUjluyd/ffff+C2Fi1alBccc2DP2ouPPSiLFg1+gnPggQfmwH16P08H7bswBx7Y+/dsb+nSpdl3Ye+Ti8V7zcvSpUsH7tMRRxyR6fachWWiPqizz555YKBfvWnFihWzqjedfvrMQUe/+lbHH3/8rOpNJ5ww80lHv3rTqafOfDLYr9707GfP/GG+X73plFNmHtDoV9/qla/8mVnVm844Y+YP+/3qTSeeOPMgQ7/6zmirzefvmc+cebC9X31ntdXma6et/eGoo2b+0Nyv3nTIIYdkmreJLFpYcsghh+yWttrc38fGxqY9+Z4/WR/EoYcemuku5vuRBRP1QR188MHZb6/evXrG3vNz8MEHD9xWW8e+JDnggAOydN/e54qH7LdXDjig9wDU9trcvoMOOihLpvnD77/Pghx00OBfCGjzfbXNc7/DDz8805xmZa95E/VBtPka7HccWTi/ZNFe87NwfsmKgxblE68/KxPfN9tRW9uXJEceeWSm+2S0YLI+qMMOOyzT7KbZe/5EfRDHHDNzaNmv3jTo36qUkk/86vNy1EGLpn0uVq6ceeC7X73puONmHhSdrj5dP2dy9NFHD7R9/V6vw+yjo27fvHnz8jdvOHuoQdiTTjpp4HV/7Md+bMb6oNv3rGc9a8Z2+tWb+vV/mO1r8/3rJS95yazqTW29dpYtWzbtVWXzJuuDGvWz6ij76GmnDT6YO9M2Ni2cX/LJNz4/8+ZNPxza1tjFkiVLMrZk75615Qf8SJYsGfwb+22dFyXtjvvts88+eenxvcdgfvLEQ7LPPvsM1E6bY35tttU1ZbpZ5/c0Y2NjdXx88ElNnq42b96c937h5lw3/nhOHVuS33nFCUO9wNtuJ0k2btyY133smqx5YH1WHro4n7jgzKECgqZNmzblbZ/6bm689/s56Yhn5E9fc9rIL/D169fnjPddmR9urtlrfsmqd58zcEDQ9OSTT+bM91+ZpzZuyT4L5+Wai84ZKiBoeuKJJ3L6+67Mpi0Ttxha/e5zhgoImh577LGc/v6rUjNxmfnqi1441IfEpg0bNuSsD1yZ9T/YnMV7z8+33nXOUAFB0yOPPDJlop9Vv/38gd9km9atWzdlkuLvvON5QwUETffee++USYq/8eunDxUQNF199dVTJin+5KsOGiogaFq7du2UiZ+ueNPJQwUETatXr54ySfHfveaIgQOCpltuuWXK5GxfuuDYoQYymm6++eYpE61+8VeeOdQgZ9N11103ZZLiz/3isqECgqZrr712yiS3f//aI4caYG664YYbpkx09vnzjxrqQ8a2n/v8ZVMmKf7zFw0XEDStWrVqyqS0n/mFw4cKCJpuuummKZO2fuGXjxlqwHRntNXm83f77bdPmVj4K284YahB/Z3VVpuvnbb2hzvvvHPK5KJfe/MpQwUETQ8++OCUSU2//fbnDjWguLPaanN/Hx8fnzJJ8dffctrAAUHTAw88MGWS4m/95plDBQRNDz300JRJfK/5rbOGGkBvauvYlySPPvrolMn4Vr/rBUN9EN6qze17+OGHp0xS/C/vPHuogKCpzffVNs/97rvvvikT137zbWcMNYC+VZuvwV7HmeXLl+eaOx/N2oeezIqD982ZRx0w7eBkU1vblyT33HPPlEmKr3rrs4cKCJruv//+KZMUX/0bzxk4IGi64447pkxS/NU3njhUQNA06N+q1tr3uVizZs2UiVa//PrjhgoImm699dYpk/j+06+u7DvAPl0/165dm5dccuO2dS6/8KQcffTRfX9u++3r9Xo+7rjjRtpHR92+LVu25NJv3ZUb7nk8pxy5JOeftTw333zzlEl8//F1Rw81gAyrsA0AAA8bSURBVN50xRVXTJnE+GMvX5TFR5829PZdf/31UyYp/odfWj5UQNB04403trZ9bb5/XX755VMmKf7YyxcNFRA0tfXaufvuu6dM8PvPv3bqUAFB06ifVXvto9dff/2USYo/e97YUAFB0/bb+LU3n5Kv3Vun/L6ZAoKmtsYuHn/88Zz+vq9nSyZCmdXvftFQAUFTW+dFSbvjfk899VRe+MdX5rENm7L/ogW56p3nDBwQNLU55tdmW08npZR7aq09P3gICQAAAAAA4GlsppBgd82TAQAAAAAA7GZCAgAAAAAA6CghAQAAAAAAdJSQAAAAAAAAOkpIAAAAAAAAHSUkAAAAAACAjhISAAAAAABARwkJAAAAAACgo4QEAAAAAADQUUICAAAAAADoKCEBAAAAAAB0lJAAAAAAAAA6SkgAAAAAAAAdJSQAAAAAAICOEhIAAAAAAEBHCQkAAAAAAKCjhAQAAAAAANBRQgIAAAAAAOgoIQEAAAAAAHSUkAAAAAAAADpKSAAAAAAAAB0lJAAAAAAAgI4SEgAAAAAAQEcJCQAAAAAAoKOEBAAAAAAA0FFCAgAAAAAA6CghAQAAAAAAdJSQAAAAAAAAOkpIAAAAAAAAHSUkAAAAAACAjhISAAAAAABARwkJAAAAAACgo4QEAAAAAADQUUICAAAAAADoKCEBAAAAAAB0lJAAAAAAAAA6SkgAAAAAAAAdJSQAAAAAAICOEhIAAAAAAEBHCQkAAAAAAKCjhAQAAAAAANBRQgIAAAAAAOgoIQEAAAAAAHSUkAAAAAAAADpKSAAAAAAAAB0lJAAAAAAAgI4SEgAAAAAAQEcJCQAAAAAAoKOEBAAAAAAA0FFCAgAAAAAA6CghAQAAAAAAdJSQAAAAAAAAOkpIAAAAAAAAHSUkAAAAAACAjhISAAAAAABARwkJAAAAAACgo1oJCUop80opf1ZKub2Uclsp5S0zrLuylPKNUsqtpZTvlFJOnnz8R0opfz/5+HdLKV8upRzbRv8AAAAAAIAdtXUlwflJTkpyXJLnJXnn1sH/Hi5J8uFa63FJPpDk443ah5McX2s9Lclnk3y0pf4BAAAAAADbaSskeE2Sj9RaN9daH0nyqSTnbb9SKeWQJGcmuXTyoU8nWVZKObbW+m+11n+stdbJ2tVJVrTUPwAAAAAAYDtthQTLk9zZWF47+dj2liW5r9a6KUkmA4G7pln3bZm4mgAAAAAAANgJFgyyUinlm0lWTlM+vb3ubPt9v5Pk2CQvnWGdtyd5+9blJUuWtN0NAAAAAAB4WhsoJKi1Pn+meinlriRHJfnm5EMrMnGFwPbuTnJ4KWVBrXVTKaVk4iqCbeuWUn4ryc8l+Yla64YZ+nRxkou3Lo+NjdXp1gUAAAAAAHbU1u2G/jbJG0op80spB2ZijoJPbb9SrfXBJKsyMdFxkrw6yXit9bZk29UB5yV5Wa31sZb6BgAAAAAA9DDQlQQD+B9JnptkTZKa5OJa6/VJUko5N8m5tdb/fXLdC5N8fPKWQt9PcsHkemNJ/iTJHUkun7jIID+otZ7VUh8BAAAAAICGMjF38J5vbGysjo+P7+5uAAAAAADAnFJKuafWOtar1tbthgAAAAAAgD2MkAAAAAAAADpKSAAAAAAAAB0lJAAAAAAAgI4SEgAAAAAAQEcJCQAAAAAAoKOEBAAAAAAA0FFCAgAAAAAA6CghAQAAAAAAdJSQAAAAAAAAOkpIAAAAAAAAHSUkAAAAAACAjhISAAAAAABARwkJAAAAAACgo4QEAAAAAADQUUICAAAAAADoKCEBAAAAAAB0lJAAAAAAAAA6SkgAAAAAAAAdJSQAAAAAAICOEhIAAAAAAEBHCQkAAAAAAKCjhAQAAAAAANBRQgIAAAAAAOgoIQEAAAAAAHSUkAAAAAAAADpKSAAAAAAAAB0lJAAAAAAAgI4SEgAAAAAAQEcJCQAAAAAAoKOEBAAAAAAA0FFCAgAAAAAA6CghAQAAAAAAdJSQAAAAAAAAOkpIAAAAAAAAHSUkAAAAAACAjhISAAAAAABARwkJAAAAAACgo4QEAAAAAADQUUICAAAAAADoKCEBAAAAAAB0lJAAAAAAAAA6SkgAAAAAAAAdJSQAAAAAAICOEhIAAAAAAEBHCQkAAAAAAKCjhAQAAAAAANBRQgIAAAAAAOgoIQEAAAAAAHSUkAAAAAAAADpKSAAAAAAAAB0lJAAAAAAAgI4SEgAAAAAAQEcJCQAAAAAAoKOEBAAAAAAA0FFCAgAAAAAA6CghAQAAAAAAdFQrIUEpZV4p5c9KKbeXUm4rpbxlhnVXllK+UUq5tZTynVLKyT3WuaCUUkspr2qjfwAAAAAAwI7aupLg/CQnJTkuyfOSvLPX4P+kS5J8uNZ6XJIPJPl4s1hKWZHkDUmubqlvAAAAAABAD22FBK9J8pFa6+Za6yNJPpXkvO1XKqUckuTMJJdOPvTpJMtKKcdO1ucl+WiStyb5QUt9AwAAAAAAemgrJFie5M7G8trJx7a3LMl9tdZNSVJrrUnuaqz79iRX1Vr/paV+AQAAAAAA01gwyEqllG8mWTlN+fQ2OlJKOSXJq5OcM+D6b89EqJAkWbJkSRvdAAAAAACAzhgoJKi1Pn+meinlriRHJfnm5EMrMnGFwPbuTnJ4KWVBrXVTKaVk4iqCu5K8bPLn1kw8nMOSfLiUcnit9YM9+nRxkou3Lo+NjdVBtgUAAAAAAJjQ1u2G/jbJG0op80spB2ZijoJPbb9SrfXBJKsyMdFxMnHlwHit9bZa6wdrrYfXWlfUWldkYuLiN/YKCAAAAAAAgNlrKyT4H0luTrImyXeSXFxrvT5JSinnllI+2lj3wiQXllJuTXJRkgta6gMAAAAAADCEMjF38J5vbGysjo+P7+5uAAAAAADAnFJKuafWOtar1taVBAAAAAAAwB5GSAAAAAAAAB0lJAAAAAAAgI4SEgAAAAAAQEcJCQAAAAAAoKOEBAAAAAAA0FFCAgAAAAAA6CghAQAAAAAAdJSQAAAAAAAAOkpIAAAAAAAAHSUkAAAAAACAjhISAAAAAABARwkJAAAAAACgo4QEAAAAAADQUUICAAAAAADoKCEBAAAAAAB0lJAAAAAAAAA6SkgAAAAAAAAdJSQAAAAAAICOEhIAAAAAAEBHCQkAAAAAAKCjhAQAAAAAANBRQgIAAAAAAOgoIQEAAAAAAHSUkAAAAAAAADpKSAAAAAAAAB0lJAAAAAAAgI4SEgAAAAAAQEcJCQAAAAAAoKOEBAAAAAAA0FFCAgAAAAAA6CghAQAAAAAAdJSQAAAAAAAAOkpIAAAAAAAAHSUkAAAAAACAjhISAAAAAABARwkJAAAAAACgo4QEAAAAAADQUUICAAAAAADoKCEBAAAAAAB0lJAAAAAAAAA6SkgAAAAAAAAdJSQAAAAAAICOEhIAAAAAAEBHlVrr7u5DK0opP0iybnf3Yw5ZnGT97u4EsEdx3ABG4dgBDMtxAxiFYwcwLMeNqZbWWvfuVXjahARMVUoZr7WO7e5+AHsOxw1gFI4dwLAcN4BROHYAw3LcGJzbDQEAAAAAQEcJCQAAAAAAoKOEBE9fF+/uDgB7HMcNYBSOHcCwHDeAUTh2AMNy3BiQOQkAAAAAAKCjXEkAAAAAAAAdJSQAAAAAAICOEhLsoUopK0sp3yil3FpK+U4p5eRp1nt9KWVNKeX2UspHSikLd3VfgbljkGNHKeXHSynfLqXcWEr5Xinlj0op3i+gwwY975hct5RSvlpKeWxX9hGYW4b4vPKsUsoVpZSbJv/93K7uKzB3DPh5ZV4p5eLJzyvXlVIuL6Ucuzv6C+x+pZT/VkpZW0qppZRnz7CeMdIZGPTZc12S5MO11uOSfCDJx7dfoZRydJLfT/LiJMcmOTTJG3dhH4G5p++xI8mjSV5baz0pyXOSvCDJ63ZZD4G5aJBjx1a/meT2XdEpYE4b5PPKoiSfTfJ7tdYTk5yS5J93ZSeBOWeQc45zk7wwyWm11lOTfCXJe3dZD4G55v9N8qIkd063gjHS/oQEe6BSyiFJzkxy6eRDn06yrEdy/vNJPldrvb9OzFD9oSTn7bqeAnPJoMeOWuvqWusdk///tyTXJlmxC7sKzCFDnHdk8tt+r0ry/l3XQ2CuGeK48YtJrq61fj1Jaq2ba63rdl1PgblkiGNHTbJ3kh8ppZQkz0gyvss6CswptdYra639jgHGSPsQEuyZliW5r9a6KUkmd+67kizfbr3lmZqire2xDtAdgx47timlHJaJN9PP75IeAnPRQMeOyct1P5LkwiSbd3UngTll0HOOk5L8oJTy+VLKtaWUT5RSlu7ivgJzx6DHjn9IckWS+5Pcl+SlSf7zrusmsAcyRtqHkACAnkopz8jECfgf1Vqv2d39Aea89yT5TK31pt3dEWCPsSDJT2QiXDw9yT1JPrhbewTsCc7MxO3JjkxyRCZuN/Sh3dojgD2ckGDPdHeSw0spC5KJCQIzkX7dtd16dyU5qrG8osc6QHcMeuxIKWW/JF9M8tla68W7tJfAXDPoseNHk7y1lLI2ydeTPGNyAjHfCobuGebzyuW11nsmvzF8aZKzd2lPgblk0GPH65J8tdb6WK11S5K/SvKSXdpTYE9jjLQPIcEeqNb6YJJVSc6ffOjVScZrrbdtt+qnk5xbSjls8s31TUk+uet6Cswlgx47SimLMxEQfLHW+ge7tpfAXDPosaPW+uJa61G11hWZmDjs+7XWFe4vDt0zxOeV/5XkuZNXLybJTyf57q7pJTDXDHHsuCPJj5dS9ppcfmWSG3ZNL4E9lDHSPsrEFzbY05RSjk/y8SQHJfl+kgtqrdeXUj6aiYk4Pje53huSXDT5Y1ckeVOtdeOu7zEwFwxy7Cil/G6S/zPJ9xo/+re11j/c1f0F5oZBzzsa669Icm2tdf9d3FVgjhji88r/luRdSbZk4nZDb6y13r17eg3sbgN+Xtk7yZ9n4ksJGzMxN8Gbaq137KZuA7tRKeWSJD+T5LAkDyd5otZ6rDHS4QgJAAAAAACgo9xuCAAAAAAAOkpIAAAAAAAAHSUkAAAAAACAjhISAAAAAABARwkJAAAAAACgo4QEAAAAAADQUUICAAAAAADoKCEBAAAAAAB0lJAAAAAAAAA66v8HA+7KDF3mvKkAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 1920x480 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K995P33epYaF"
      },
      "source": [
        "###sst-2 binary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSZpp019pag4",
        "outputId": "94520a2f-b544-4668-d4a5-c754c5713f68"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGBEmdIMHUTx"
      },
      "source": [
        "#downloading binary sst-2\n",
        "data_dir = tf.keras.utils.get_file(\n",
        "    fname='SST-2.zip', \n",
        "    origin='https://dl.fbaipublicfiles.com/glue/data/SST-2.zip',\n",
        "    extract=True)\n",
        "data_dir = os.path.splitext(data_dir)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLn7KoKcadMU"
      },
      "source": [
        "class sst_binary_dataset(Dataset):\n",
        "  def __init__(self, dataframe):\n",
        "    self.df = dataframe\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.df)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    return {'input_ids': self.df.iloc[idx]['input_ids'], \n",
        "            'attention_mask': self.df.iloc[idx]['attention_mask'],\n",
        "            'label': self.df.iloc[idx]['label']}\n",
        "\n",
        "class Data_collator():\n",
        "  def __init__(self, tokenizer): \n",
        "    self.tokenizer = tokenizer\n",
        "\n",
        "  def __call__(self, batch):\n",
        "    #Assumes we are wotking with a list of dictionaries\n",
        "    training_dict = {'input_ids': [element['input_ids'] for element in batch], \n",
        "                     'attention_mask': [element['attention_mask'] for element in batch]}\n",
        "    training_samples = self.tokenizer.pad(training_dict, return_tensors = 'pt')\n",
        "    labels = torch.tensor([element['label'] for element in batch], dtype=torch.float32)\n",
        "\n",
        "    return {'input_ids': training_samples['input_ids'],\n",
        "            'attention_mask': training_samples['attention_mask'],\n",
        "            'labels': labels}\n",
        "\n",
        "def tokenizer_map_df(element):\n",
        "  #sst-2 tokenizer_df\n",
        "  tokenized_elemen = tokenizer(element['sentence'], max_length=tokenizer.model_max_length, truncation=True)\n",
        "  return element.append(pd.Series([tokenized_elemen['input_ids'], tokenized_elemen['attention_mask']], index=['input_ids', 'attention_mask']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRd47fMnal6l"
      },
      "source": [
        "#defining tokenizer and dcollator\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased') #using ver. 4.10.0.dev0\n",
        "collate_fn = Data_collator(tokenizer)\n",
        "\n",
        "#loading dataframes\n",
        "train_df = pd.read_csv(os.path.join(data_dir, 'train.tsv'), sep='\\t')\n",
        "dev_df = pd.read_csv(os.path.join(data_dir, 'dev.tsv'), sep='\\t')\n",
        "test_df = pd.read_csv(os.path.join(data_dir, 'test.tsv'), sep='\\t')\n",
        "\n",
        "#tokenizing data ---only train data\n",
        "tokenized_train_df = train_df.apply(tokenizer_map_df, axis=1)\n",
        "tokenized_train_df = tokenized_train_df[['sentence', 'input_ids', 'attention_mask', 'label']]  #changing columns order\n",
        "\n",
        "#dataset and dataloader ---only train data\n",
        "train_dset = sst_binary_dataset(tokenized_train_df)\n",
        "train_loader = DataLoader(train_dset, shuffle=True, batch_size=16, collate_fn=collate_fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUYe-Gv1meoF",
        "outputId": "35f3ab5d-3329-42b2-809f-0b0b8d937859"
      },
      "source": [
        "#Loading model and tokenizer\n",
        "#model = DistilBertClassifier(backbone_weights=checkpoint['model']).to(device) #no frozen layers first\n",
        "model = DistilBertClassifier(backbone_weights=checkpoint['model'], freeze_backbone=True).to(device) #freezeing layers\n",
        "\n",
        "#defining loss and optimizer\n",
        "optim = AdamW(model.parameters(), lr=1e-5)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "num_epochs = 3\n",
        "num_training_steps = num_epochs * len(train_loader)\n",
        "lr_scheduler = get_scheduler('linear', optimizer=optim, num_warmup_steps=600, num_training_steps=num_training_steps)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights loaded succesfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QBmHCjUH25o"
      },
      "source": [
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "  epoch_loss = 0.0\n",
        "  running_loss = 0.0\n",
        "  for i, batch in enumerate(train_loader):\n",
        "    input_ids, attention_mask, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['labels']\n",
        "    optim.zero_grad()\n",
        "\n",
        "    out = model(input_ids, attention_mask, return_hidden_embeddings=False)\n",
        "    loss = loss_fn(out, labels.type(torch.LongTensor).to(device))\n",
        "\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "    lr_scheduler.step()\n",
        "\n",
        "    epoch_loss+=loss.item()\n",
        "    running_loss+=loss.item()\n",
        "    if i!=0 and i%2000 == 0:\n",
        "      print('saving model at iter{}'.format(i))\n",
        "      torch.save({'epoch': epoch,\n",
        "                  'model_state_dict':model.state_dict(),\n",
        "                  'optimizer_state_dict':optim.state_dict(),\n",
        "                  'lr_scheduler':lr_scheduler.state_dict(),\n",
        "                  'running_loss':running_loss/2000},\n",
        "                  './distilbert-frozenweights-sst-binary-shuffle-true/Distilbert_reddit_sst_binary_epoch_{}_iter_{}.pt'.format(epoch, i))\n",
        "      \n",
        "      print('epoch: {}_final running_loss: {}'.format(epoch, running_loss/2000))\n",
        "      running_loss = 0.0\n",
        "      \n",
        "    if i!=0 and i%100==0:\n",
        "      print('epoch: {} iter: {} last_batch_loss: {}'.format(epoch, i, loss.item()))\n",
        "\n",
        "  epoch_loss = epoch_loss/len(train_loader)\n",
        "  print('storing \"end of the epoch\" weights epoch_loss: {}'.format(epoch_loss))\n",
        "  torch.save({'epoch': epoch,\n",
        "              'model_state_dict':model.state_dict(),\n",
        "              'optimizer_state_dict':optim.state_dict(),\n",
        "              'lr_scheduler':lr_scheduler.state_dict(),\n",
        "              'epoch_loss':epoch_loss},\n",
        "              './distilbert-frozenweights-sst-binary-shuffle-true/Distilbert_reddit_sst_binary_epoch_{}.pt'.format(epoch))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJb6n00Uur2x"
      },
      "source": [
        "###Financial Phrasebank\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a79nSE2Zu4O_"
      },
      "source": [
        "import requests\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJsm5fY_59DL"
      },
      "source": [
        "#downloading data\n",
        "url = 'https://www.researchgate.net/profile/Pekka-Malo/publication/251231364_FinancialPhraseBank-v10/data/0c96051eee4fb1d56e000000/FinancialPhraseBank-v10.zip'\n",
        "r = requests.get(url)\n",
        "with open('./FinancialPhraseBank-v10.zip', 'wb') as file:\n",
        "  file.write(r.content)\n",
        "!unzip ./FinancialPhraseBank-v10.zip -d FinancialPhraseBank-v10\n",
        "!rm ./FinancialPhraseBank-v10.zip"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3IZc7IXu_Eg"
      },
      "source": [
        "def tokenizer_map_df(element):\n",
        "  #sst-2 tokenizer_df\n",
        "  tokenized_elemen = tokenizer(element['sentence'], max_length=tokenizer.model_max_length, truncation=True)\n",
        "  return element.append(pd.Series([tokenized_elemen['input_ids'], tokenized_elemen['attention_mask']], index=['input_ids', 'attention_mask']))\n",
        "\n",
        "class fp_dataset(Dataset):\n",
        "  def __init__(self, dataframe):\n",
        "    self.df = dataframe\n",
        "  def __len__(self):\n",
        "    return len(self.df)\n",
        "  def __getitem__(self, idx): \n",
        "    return {'input_ids': self.df['input_ids'].iloc[idx],\n",
        "            'attention_mask': self.df['attention_mask'].iloc[idx],\n",
        "            'label': self.df['label'].iloc[idx]}\n",
        "\n",
        "class Data_collator():\n",
        "  def __init__(self, tokenizer): \n",
        "    self.tokenizer = tokenizer\n",
        "\n",
        "  def __call__(self, batch):\n",
        "    #Assumes we are wotking with a list of dictionaries\n",
        "    training_dict = {'input_ids': [element['input_ids'] for element in batch], \n",
        "                     'attention_mask': [element['attention_mask'] for element in batch]}\n",
        "    training_samples = self.tokenizer.pad(training_dict, return_tensors = 'pt')\n",
        "    labels = torch.tensor([element['label'] for element in batch], dtype=torch.float32)\n",
        "\n",
        "    return {'input_ids': training_samples['input_ids'],\n",
        "            'attention_mask': training_samples['attention_mask'],\n",
        "            'labels': labels}"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vu3iTm7EvC8f"
      },
      "source": [
        "#Loading financial phrasebank dataset\n",
        "#fp_dataframe_allagree = pd.read_csv('./FinancialPhraseBank-v10/FinancialPhraseBank-v1.0/Sentences_AllAgree.txt', sep='@', names=['sentence', 'label'], engine='python')\n",
        "fp_dataframe_50agree = pd.read_csv('./FinancialPhraseBank-v10/FinancialPhraseBank-v1.0/Sentences_50Agree.txt', sep='@', names=['sentence', 'label'], engine='python')\n",
        "\n",
        "#mapping data - allagree  ---may wanna use 50agree to increase data size\n",
        "label_mapping = {'negative':0, 'neutral':1, 'positive':2}\n",
        "fp_dataframe_50agree['label'] = fp_dataframe_50agree['label'].map(label_mapping)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMWGDqWNvHZf"
      },
      "source": [
        "#defining tokenizer and Data Collator\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased') #using ver. 4.10.0.dev0\n",
        "collate_fn = Data_collator(tokenizer)\n",
        "\n",
        "#tokenizing data\n",
        "tokenized_fp_dataframe_50agree = fp_dataframe_50agree.apply(tokenizer_map_df, axis=1)\n",
        "tokenized_fp_dataframe_50agree = tokenized_fp_dataframe_50agree[['sentence', 'input_ids', 'attention_mask', 'label']]\n",
        "\n",
        "#splitting dataframe\n",
        "fp_train, fp_test = train_test_split(tokenized_fp_dataframe_50agree, test_size=0.2, random_state=2332)\n",
        "fp_train, fp_val = train_test_split(fp_train, test_size=0.1, random_state=2332)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nUi0Lwr4pi6"
      },
      "source": [
        "#datasets+dataloaders\n",
        "fp_dataset_train = fp_dataset(fp_train)\n",
        "fp_dataset_test = fp_dataset(fp_test)\n",
        "fp_dataset_val = fp_dataset(fp_val)\n",
        "\n",
        "train_loader = DataLoader(fp_dataset_train, shuffle=True, batch_size=16, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(fp_dataset_test, shuffle=True, batch_size=16, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(fp_dataset_val, shuffle=True, batch_size=16, collate_fn=collate_fn)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFbTWgTZ8AxU"
      },
      "source": [
        "###---Training\n",
        "\n",
        "#Loading model \n",
        "model = DistilBertClassifier(backbone_weights=checkpoint['model'], out_dim=3).to(device) #no frozen layers first\n",
        "#model = DistilBertClassifier(backbone_weights=checkpoint['model'],  out_dim=3, freeze_backbone=True).to(device) #freezeing layers\n",
        "\n",
        "#defining loss and optimizer\n",
        "optim = AdamW(model.parameters(), lr=1e-5)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "num_epochs = 3\n",
        "num_training_steps = num_epochs * len(train_loader)\n",
        "lr_scheduler = get_scheduler('linear', optimizer=optim, num_warmup_steps=600, num_training_steps=num_training_steps)\n",
        "\n",
        "\n",
        "\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "  epoch_loss = 0.0\n",
        "  running_loss = 0.0\n",
        "  for i, batch in enumerate(train_loader):\n",
        "    input_ids, attention_mask, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['labels']\n",
        "    optim.zero_grad()\n",
        "\n",
        "    out = model(input_ids, attention_mask, return_hidden_embeddings=False)\n",
        "    loss = loss_fn(out, labels.type(torch.LongTensor).to(device))\n",
        "\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "    lr_scheduler.step()\n",
        "\n",
        "    epoch_loss+=loss.item()\n",
        "    running_loss+=loss.item()\n",
        "    if i!=0 and i%2000 == 0:\n",
        "      print('saving model at iter{}'.format(i))\n",
        "      torch.save({'epoch': epoch,\n",
        "                  'model_state_dict':model.state_dict(),\n",
        "                  'optimizer_state_dict':optim.state_dict(),\n",
        "                  'lr_scheduler':lr_scheduler.state_dict(),\n",
        "                  'running_loss':running_loss/2000},\n",
        "                  './distilbert-reddit-financial-phrasebank-3-epochs/Distilbert_reddit_financial_phrasebank_epoch_{}_iter_{}.pt'.format(epoch, i))\n",
        "      \n",
        "      print('epoch: {}_final running_loss: {}'.format(epoch, running_loss/2000))\n",
        "      running_loss = 0.0\n",
        "      \n",
        "    if i!=0 and i%100==0:\n",
        "      print('epoch: {} iter: {} last_batch_loss: {}'.format(epoch, i, loss.item()))\n",
        "\n",
        "  epoch_loss = epoch_loss/len(train_loader)\n",
        "  print('storing \"end of the epoch\" weights epoch_loss: {}'.format(epoch_loss))\n",
        "  torch.save({'epoch': epoch,\n",
        "              'model_state_dict':model.state_dict(),\n",
        "              'optimizer_state_dict':optim.state_dict(),\n",
        "              'lr_scheduler':lr_scheduler.state_dict(),\n",
        "              'epoch_loss':epoch_loss},\n",
        "              './distilbert-reddit-financial-phrasebank-3-epochs/Distilbert_reddit_financial_phrasebank_epoch_{}.pt'.format(epoch))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5dEHe4J09Pv"
      },
      "source": [
        "####Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHbCTzIOyuPJ"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "#loading weights\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = '/content'\n",
        "!kaggle datasets download -d muniozdaniel0/distilbert-reddit-financial-phrasebank-allagree\n",
        "!unzip ./distilbert-reddit-financial-phrasebank-allagree.zip -d ./distilbert-reddit-fp-allagree\n",
        "!rm ./distilbert-reddit-financial-phrasebank-allagree.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKccwswvHgVP"
      },
      "source": [
        "def testing(model, data_loader, return_probs=False, keep_training=False, verbose=False):\n",
        "  loss = 0.0\n",
        "  accuracy = 0.0\n",
        "\n",
        "  labels = torch.tensor([], dtype=torch.double)\n",
        "  predictions = torch.tensor([], dtype=torch.double)\n",
        "  probabilities = torch.tensor([], dtype=torch.double)\n",
        "  \n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for i, elemen in enumerate(data_loader):\n",
        "      out_model = model(elemen['input_ids'], elemen['attention_mask'])\n",
        "      batch_loss = F.cross_entropy(out_model, elemen['labels'].type(torch.LongTensor))\n",
        "\n",
        "      probs = F.softmax(out_model, dim=1)\n",
        "      vals, idx = probs.topk(1, dim=1)\n",
        "      acc = sum(idx.squeeze() == elemen['labels'].type(torch.LongTensor))/len(idx.squeeze())\n",
        "\n",
        "      labels = torch.cat((labels, elemen['labels'].type(torch.LongTensor)))\n",
        "      predictions = torch.cat((predictions, idx.squeeze()))\n",
        "      probabilities = torch.cat((probabilities, vals.squeeze()))\n",
        "\n",
        "      loss+=batch_loss.item()\n",
        "      accuracy+=acc.item()\n",
        "\n",
        "      if verbose:\n",
        "        #print('step: {}, accuracy: {}, loss: {}'.format(i+1, accuracy/(i+1), loss/(i+1)))\n",
        "        print('step: {}, batch_loss: {}, batch_accuracy: {}'.format(i+1, batch_loss.item(), acc.item()))\n",
        "      \n",
        "  loss/=len(data_loader)\n",
        "  accuracy/=len(data_loader)\n",
        "  print('---- Total accuracy: {}, Total loss: {}'.format(accuracy, loss))\n",
        "  if keep_training:\n",
        "    model.train()\n",
        "\n",
        "  if not return_probs:\n",
        "    return loss, accuracy, labels, predictions\n",
        "  else:\n",
        "    return loss, accuracy, labels, predictions, probabilities\n",
        "\n",
        "\n",
        "def get_metrics(actual_values, predicted_values, index):\n",
        "  #casting to make sure everything is alright\n",
        "  actual_values = actual_values.type(torch.LongTensor).numpy()\n",
        "  predicted_values = predicted_values.type(torch.LongTensor).numpy()\n",
        "\n",
        "  gt_values = np.ma.masked_equal(actual_values, index).mask\n",
        "  mp_values = np.ma.masked_equal(predicted_values, index).mask\n",
        "\n",
        "  precision = sum(mp_values & gt_values)/sum(mp_values)\n",
        "  recall = sum(mp_values & gt_values)/sum(gt_values)\n",
        "  F1 = 2*((precision*recall)/(precision+recall))\n",
        "\n",
        "  return precision, recall, F1"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBAQkavzQYFW"
      },
      "source": [
        "#loading datasets\n",
        "testing_train_dloader = DataLoader(fp_dataset_train, shuffle=False, batch_size=16, collate_fn=collate_fn)\n",
        "testing_test_dloader = DataLoader(fp_dataset_test, shuffle=False, batch_size=16, collate_fn=collate_fn)\n",
        "testing_val_dloader = DataLoader(fp_dataset_val, shuffle=False, batch_size=16, collate_fn=collate_fn)\n",
        "\n",
        "#loading checkpoints\n",
        "checkpoint_3_epochs = torch.load('./distilbert-reddit-fp-allagree/Distilbert_reddit_financial_phrasebank_3_epochs.pt', map_location=device)\n",
        "checkpoint_5_epochs = torch.load('./distilbert-reddit-fp-allagree/Distilbert_reddit_financial_phrasebank_5_epochs.pt', map_location=device)\n",
        "checkpoint_7_epochs = torch.load('./distilbert-reddit-fp-allagree/Distilbert_reddit_financial_phrasebank_7_epochs.pt', map_location=device)\n",
        "checkpoint_10_epochs = torch.load('./distilbert-reddit-fp-allagree/Distilbert_reddit_financial_phrasebank_10_epochs.pt', map_location=device)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vv_P5LZ9Ir2a",
        "outputId": "ffc334de-96a6-4123-923d-b70a7521cdf9"
      },
      "source": [
        "##evaluating '3_epochs'\n",
        "model = DistilBertClassifier(out_dim=3)\n",
        "model.load_state_dict(checkpoint_3_epochs['model_state_dict'])\n",
        "\n",
        "print('\\n-----train')\n",
        "train_loss, train_acc, train_true_values, train_preds = testing(model, testing_train_dloader)\n",
        "print('\\n-----test')\n",
        "test_loss, test_acc, test_true_values, test_preds = testing(model, testing_test_dloader)\n",
        "print('\\n-----val')\n",
        "val_loss, val_acc, val_true_values, val_preds = testing(model, testing_val_dloader)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-----train\n",
            "---- Total accuracy: 0.9217316513761468, Total loss: 0.2329277686364607\n",
            "\n",
            "-----test\n",
            "---- Total accuracy: 0.8575819672131147, Total loss: 0.34995327657852016\n",
            "\n",
            "-----val\n",
            "---- Total accuracy: 0.8275, Total loss: 0.4211508938670158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BuRiwaLp89Za",
        "outputId": "302c9995-44f2-4f8f-d824-244d630e11c1"
      },
      "source": [
        "##evaluating weights '5_epochs'\n",
        "model = DistilBertClassifier(out_dim=3)\n",
        "model.load_state_dict(checkpoint_5_epochs['model_state_dict'])\n",
        "\n",
        "print('\\n-----train')\n",
        "train_loss_5, train_acc_5, train_true_values_5, train_preds_5 = testing(model, testing_train_dloader)\n",
        "print('\\n-----test')\n",
        "test_loss_5, test_acc_5, test_true_values_5, test_preds_5 = testing(model, testing_test_dloader)\n",
        "print('\\n-----val')\n",
        "val_loss_5, val_acc_5, val_true_values_5, val_preds_5 = testing(model, testing_val_dloader)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-----train\n",
            "---- Total accuracy: 0.96875, Total loss: 0.11476771865005887\n",
            "\n",
            "-----test\n",
            "---- Total accuracy: 0.8620901635435761, Total loss: 0.3613927063883328\n",
            "\n",
            "-----val\n",
            "---- Total accuracy: 0.8225, Total loss: 0.4521456964313984\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80Evb4XXXMNo",
        "outputId": "6243e49b-961d-40e5-c00a-330fa761ae15"
      },
      "source": [
        "##evaluating weights '7_epochs'\n",
        "model = DistilBertClassifier(out_dim=3)\n",
        "model.load_state_dict(checkpoint_7_epochs['model_state_dict'])\n",
        "\n",
        "print('\\n-----train')\n",
        "train_loss_7, train_acc_7, train_true_values_7, train_preds_7 = testing(model, testing_train_dloader)\n",
        "print('\\n-----test')\n",
        "test_loss_7, test_acc_7, test_true_values_7, test_preds_7 = testing(model, testing_test_dloader)\n",
        "print('\\n-----val')\n",
        "val_loss_7, val_acc_7, val_true_values_7, val_preds_7 = testing(model, testing_val_dloader)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-----train\n",
            "---- Total accuracy: 0.9899655963302753, Total loss: 0.04829801462266013\n",
            "\n",
            "-----test\n",
            "---- Total accuracy: 0.869262294691117, Total loss: 0.4218116987374474\n",
            "\n",
            "-----val\n",
            "---- Total accuracy: 0.8325, Total loss: 0.538484151288867\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJuPz_I5XOap",
        "outputId": "8b6f4692-7efc-4865-fca5-1f8bc06b1267"
      },
      "source": [
        "##evaluating weights '10_epochs'\n",
        "model = DistilBertClassifier(out_dim=3)\n",
        "model.load_state_dict(checkpoint_10_epochs['model_state_dict'])\n",
        "\n",
        "print('\\n-----train')\n",
        "train_loss_10, train_acc_10, train_true_values_10, train_preds_10 = testing(model, testing_train_dloader)\n",
        "print('\\n-----test')\n",
        "test_loss_10, test_acc_10, test_true_values_10, test_preds_10 = testing(model, testing_test_dloader)\n",
        "print('\\n-----val')\n",
        "val_loss_10, val_acc_10, val_true_values_10, val_preds_10 = testing(model, testing_val_dloader)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-----train\n",
            "---- Total accuracy: 0.9954128440366973, Total loss: 0.02407797688293621\n",
            "\n",
            "-----test\n",
            "---- Total accuracy: 0.8631147537075106, Total loss: 0.4852838981896639\n",
            "\n",
            "-----val\n",
            "---- Total accuracy: 0.83, Total loss: 0.6317341728135943\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3M-uDFkxeF_"
      },
      "source": [
        "testing_df = pd.DataFrame(columns=['train', 'test', 'evaluation'])\n",
        "testing_df.loc['3_epochs'] = [train_acc, test_acc, val_acc]\n",
        "testing_df.loc['5_epochs'] = [train_acc_5, test_acc_5, val_acc_5]\n",
        "testing_df.loc['7_epochs'] = [train_acc_7, test_acc_7, val_acc_7]\n",
        "testing_df.loc['10_epochs'] = [train_acc_10, test_acc_10, val_acc_10]"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        },
        "id": "CbUOU2lAxg-K",
        "outputId": "f410fe5c-ea0e-4b32-8958-b9c8fb77b02c"
      },
      "source": [
        "#may need to double test on glue\n",
        "testing_df.round(2)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>train</th>\n",
              "      <th>test</th>\n",
              "      <th>evaluation</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3_epochs</th>\n",
              "      <td>0.92</td>\n",
              "      <td>0.86</td>\n",
              "      <td>0.83</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5_epochs</th>\n",
              "      <td>0.97</td>\n",
              "      <td>0.86</td>\n",
              "      <td>0.82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7_epochs</th>\n",
              "      <td>0.99</td>\n",
              "      <td>0.87</td>\n",
              "      <td>0.83</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10_epochs</th>\n",
              "      <td>1.00</td>\n",
              "      <td>0.86</td>\n",
              "      <td>0.83</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           train  test  evaluation\n",
              "3_epochs    0.92  0.86        0.83\n",
              "5_epochs    0.97  0.86        0.82\n",
              "7_epochs    0.99  0.87        0.83\n",
              "10_epochs   1.00  0.86        0.83"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SM5alTqOyCGM"
      },
      "source": [
        ""
      ],
      "execution_count": 75,
      "outputs": []
    }
  ]
}